{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053535bf-73df-4b63-9c84-356c701136ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d5c91-e135-4452-960e-8e37aa4fa54d",
   "metadata": {},
   "source": [
    "#### fix for could not import -lmza\n",
    "https://github.com/lucidrains/imagen-pytorch/issues/92 \n",
    "\n",
    "#### Fix for Can't import datasets AttributeError: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)\n",
    "\n",
    "Restart juypyter and kernel\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921b8a86-e2c6-40fa-9e28-a597eb2bf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f87d5a5-e6ae-4234-914a-5f294b387b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "GPU 0:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import time\n",
    "helper.check_gpu_capacity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d9cd78-87e0-4456-81f7-e94932af0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1d699b-cc24-46fd-b0ab-f25871734a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'response'],\n",
       "    num_rows: 687\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_from_disk(\"generated-dataset/llama-7-int4-dolly-filtered\")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb913e1-0e58-4a35-8630-8e87fd15cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> instruction\n",
      "====================\n",
      "What is the procedure for managing the PDM system?\n",
      "====================\n",
      "--> response\n",
      "====================\n",
      "Process\n",
      "\n",
      "\n",
      "Fill in full service analysis form  Request form can be found in the E-WorkBook, under Group Collaboration –\n",
      "                                    FST-QMS – FST-QM – 1.0 Quality Manual – 4. Forms and Documents – FST420 (for powder samples) or FST421 (for liquid samples)\n",
      "                                    Please always use the latest version of the request form available at E-workbook!\n",
      "                                    Request form for powder and liquid are different\n",
      "Submit request form                 Sent to info.fstlab@danone.com and attached the request form\n",
      "                                    Request should be sent at least a week before, preferably before Thursday because the FST lab team need to make the schedule\n",
      "Send in the samples                 Samples can be put in the long table in the lab (behind the table with microscopes) with clear label indicating Name of PD, code of sample, and date of analysis.\n",
      "Delivery of the results             Results will be sent by email and can be found in the E-WorkBook, under\n",
      "                                    FST-Analyses results - Full service analyses - name of your project\n",
      "Questions on the results            Contact FST lab team working on the analysis (information can be found in the E-WorkBook)\n",
      "\n",
      "Record of revisions\n",
      "\n",
      "Version    Date        Nature of Revision\n",
      "1          04-03-2015  First version\n",
      "2          16-11-2017  Addition of reference engine guidance docs for hydrolysates\n",
      "\n",
      "\n",
      "Approval\n",
      "\n",
      "Function                                Name                Approval date\n",
      "PDM Manager /                           Fred Rasing\n",
      "Manager PD Quality & Processes\n",
      "Global R&D Director IMF Product Design  Alexandra Boelrijk\n",
      "\n",
      "Date of implementation\n",
      "\n",
      "Date of implementation: 01-04-2015\n",
      "\n",
      "Printed: 17 November 2017, 11:23 AM\tdraft\tPDM-PR.01 Procedure System Management\n",
      "\n",
      "\n",
      "SOP <Title>\t\tPage 1 of 3\n",
      "Version 1 (draft1); 21-06-2005\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "sample_ds = dataset[randrange(len(dataset))]\n",
    "\n",
    "for key, value in sample_ds.items():\n",
    "    print(f\"--> {key}\")\n",
    "    print(\"====================\")\n",
    "    print(value)\n",
    "    print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48905876-1a02-40cb-b4c6-1e8f09b8130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(filter(lambda x :x != '', list(map(lambda x:((x['category'], len(x['context'])) if len(x['context']) > 5000 else ''), dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde2f4f-f086-4e18-90ca-6871834b9a51",
   "metadata": {},
   "source": [
    "Based on our use case, we want to focus on 'general_qa' and  'information_extraction tasks.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e70e37-693e-4541-9d5c-9c36391d9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/ai-project/generated-dataset/llama-7-int4-dolly-filtered/cache-e62e99f4975fd97d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'response'],\n",
       "    num_rows: 687\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x['instruction'] != '')\n",
    "\n",
    "# print(set(dataset['instruction']))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d8c2c9-415b-4214-b2fd-81e4f489df25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> instruction\n",
      "====================\n",
      "Given the reference text, what is the insight in processing and handling of B Breve in products and processes\n",
      "====================\n",
      "--> response\n",
      "====================\n",
      "Table 3. Overview of Danone Clinical studies, the used dosages per study product and main study outcome\n",
      "\n",
      "Clinical Study    Age                                        cfu/100ml    cfu/g powder    Daily intake (cfu/day)    Study outcome\n",
      "SYNBAD            5-8 mo                                     1,30E+09     9,63E+07        1,00E+10                  Treatment of IgE mediated AD\n",
      "ATOS              1-5 mo                                     1,30E+09     9,63E+07        9,50E+09                  Safety, Growth equivalence\n",
      "SMILE             1-3 yrs                                    3,40E+08     1,80+E07        2,10E+09                  Gut microbiota development can be modulated during toddlerhood\n",
      "JULIUS            0-4 mo                                     7,50E+08     5,56E+07        2,50E+09                  Restoration of the delayed colonisation\n",
      "CT0131            0-15 days (growth study healthy infants)   1,47E+09     1,00E+08                                  Safety, growth, hypo-allergenic,\n",
      "CT0141            0-3yrs (hypoallergenicity)                                                                        Safety, growth in CMA infants and  effective in CMA management\n",
      "                  0-8 mo\n",
      "ASSIGN I          0-13 mo                                    1,47E+09     1,00E+08                                  Gut microbiota development\n",
      "ColoR-Low         0-4 mo                                     1,00E+06     7,40E+04        9,00E+06                  Effect on the microbiota (ongoing)\n",
      "ColoR-High        0-4 mo                                     1,00E+04     7,40E+06        9,00E+08                  Effect on the microbiota (ongoing)\n",
      "\n",
      "Insight in processing and handling of B Breve in products and processes\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "sample_ds = dataset[randrange(len(dataset))]\n",
    "\n",
    "for key, value in sample_ds.items():\n",
    "    print(f\"--> {key}\")\n",
    "    print(\"====================\")\n",
    "    print(value)\n",
    "    print(\"====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e7f4e-a563-4bfa-ac70-f8dcf30e2f70",
   "metadata": {},
   "source": [
    "Split the dataset into train, eval, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f3b7b5-e985-4a3a-933d-a3157e5bf8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ubuntu/ai-project/generated-dataset/llama-7-int4-dolly-filtered/cache-ac25ce4f41b2fe27.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 687\n",
      "Train size: 494\n",
      "Eval size: 124\n",
      "Test size: 69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "shuffled_dataset = shuffled_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = shuffled_dataset['train']\n",
    "test_dataset = shuffled_dataset['test']\n",
    "\n",
    "train_eval_dataset = train_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_eval_dataset['train']\n",
    "eval_dataset = train_eval_dataset['test']\n",
    "\n",
    "\n",
    "# Print the sizes of the train, eval, andtest sets\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fbfd21-dd46-4044-95d1-f91d4f3374cb",
   "metadata": {},
   "source": [
    "### Preprocess the Dialog-Summary Dataset\n",
    "We need to convert the instruction-response (prompt-response) pairs into explicit instructions for the LLM. \n",
    "\n",
    "Prepend an instruction to the start of the dialog with Answer this question and to the start of the question with question as follows:\n",
    "\n",
    "Training prompt (Question):\n",
    "Answer this question.\n",
    "\n",
    "    What is the capital of France?\n",
    "\n",
    "Answer: \n",
    "Training response (answer):\n",
    "\n",
    "    The capital of France is Paris.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61d7c21e-73a2-4271-9b61-d13cf47e6820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cb42d573854077a8d8d91232d3210c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m     sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example\n\u001b[0;32m---> 15\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m tokenized_train_dataset\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3087\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3081\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3082\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3087\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3088\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3089\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3463\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3459\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3460\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3461\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3462\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3463\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3467\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3472\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3344\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3343\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3344\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3346\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3347\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3348\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(sample):\n\u001b[1;32m      2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m### Instruction:\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m            Answer this question.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m            \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m     sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(prompt, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     11\u001b[0m     sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize_function(sample):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "            Answer this question.\n",
    "            \n",
    "            ### Question:\n",
    "            {sample['instruction']}\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    sample['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    sample['labels'] = tokenizer(sample[\"response\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['instruction', 'response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "700ac173-5482-4dc8-b832-a49d0f0bac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "            \n",
    "            ### Input:\n",
    "            {sample['response']}\n",
    "\n",
    "            ### Response:\n",
    "            {sample['instruction']}\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25d8e6-e589-48a3-9aee-6b03d66389c4",
   "metadata": {},
   "source": [
    "Let's test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e89814-1a71-474c-babc-9c398972125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
      "            \n",
      "            ### Input:\n",
      "            Food allergens trigger allergic symptoms, mainly by binding to antibodies such as immunoglobulin E (IgE). Several techniques can be used to reduce the antigenicity of infant formulas. To date, the most successful way of reducing the antigenicity of milk proteins is to hydrolyse the protein using proteolytic enzymes, resulting in peptides and free amino acids. Any remaining peptides, with larger lengths than desired, can be removed via ultrafiltration (UF), which is carried out during processing of eHP. \n",
      "\n",
      "Extensively hydrolysed products are shown to be effective in preventing allergic reactions via food challenges. According to the guidelines of the American Association of Pediatrics, at least 90% of cow’s milk allergic children should tolerate a hypoallergenic formula with a confidence interval of 95%. The eHP described in this document meet this criteria as measured by a food challenge carried out by Giampietro et al. (Giampietro, Kjellman et al. 2001). \n",
      "\n",
      "Engine\n",
      "\n",
      "\n",
      "\n",
      "The engine of eHP linked to the benefits described above is the extensively hydrolyzed protein hydrolysate as tested in several in vitro and in vivo studies and produced in-house. The 21XXXX recipes, the raw materials included in these recipes and the processing of these 21XXXX recipes are considered specific for the engine and therefore need to remain unaltered to secure engine composition and efficacy. The engine is included in the base powder and finally in the end product according to the schema depicted in Figure 1.\n",
      "\n",
      "\n",
      "Figure 1: An illustration of how the eHP engine is incorporated in the base powder and final product\n",
      "\n",
      "            ### Response:\n",
      "            1. What is the engine of eHP?\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_instruction(dataset[randrange(len(dataset))]))\n",
    "\n",
    "## Do I need to add the context in the format_instruction?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c9519a9-759d-4a77-92f7-c7f4b8534bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
      "            \n",
      "            ### Input:\n",
      "            Table 3. Overview of Danone Clinical studies, the used dosages per study product and main study outcome\n",
      "\n",
      "Clinical Study    Age                                        cfu/100ml    cfu/g powder    Daily intake (cfu/day)    Study outcome\n",
      "SYNBAD            5-8 mo                                     1,30E+09     9,63E+07        1,00E+10                  Treatment of IgE mediated AD\n",
      "ATOS              1-5 mo                                     1,30E+09     9,63E+07        9,50E+09                  Safety, Growth equivalence\n",
      "SMILE             1-3 yrs                                    3,40E+08     1,80+E07        2,10E+09                  Gut microbiota development can be modulated during toddlerhood\n",
      "JULIUS            0-4 mo                                     7,50E+08     5,56E+07        2,50E+09                  Restoration of the delayed colonisation\n",
      "CT0131            0-15 days (growth study healthy infants)   1,47E+09     1,00E+08                                  Safety, growth, hypo-allergenic,\n",
      "CT0141            0-3yrs (hypoallergenicity)                                                                        Safety, growth in CMA infants and  effective in CMA management\n",
      "                  0-8 mo\n",
      "ASSIGN I          0-13 mo                                    1,47E+09     1,00E+08                                  Gut microbiota development\n",
      "ColoR-Low         0-4 mo                                     1,00E+06     7,40E+04        9,00E+06                  Effect on the microbiota (ongoing)\n",
      "ColoR-High        0-4 mo                                     1,00E+04     7,40E+06        9,00E+08                  Effect on the microbiota (ongoing)\n",
      "\n",
      "Insight in processing and handling of B Breve in products and processes\n",
      "\n",
      "            ### Response:\n",
      "            Given the reference text, what is the insight in processing and handling of B Breve in products and processes\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(format_instruction(sample_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0156c-85de-4e8a-aeb0-8f7f8d785212",
   "metadata": {},
   "source": [
    "### Instruction-tune Llama 2 using trl and the SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ec40fdb-1621-42a2-901c-71b8a287d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "GPU 0:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "# COMMENT IN TO USE FLASH ATTENTION\n",
    "# replace attention with flash attention\n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
    "#     print(\"Using flash attention\")\n",
    "#     replace_attn_with_flash_attn()\n",
    "#     use_flash_attention = True\n",
    "\n",
    "helper.check_gpu_capacity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf02c98-762b-4fc6-89ec-0fd1c7741cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53d29288-3392-4e22-952b-d39d271af9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "model_llama_2_7b_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063f072c-398d-4c55-ae4a-ebaa8994b56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c133d81de2471c999c33be3ee23c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_llama_2_7b = AutoModelForCausalLM.from_pretrained(model_llama_2_7b_id, \n",
    "                                                        quantization_config=bnb_config, \n",
    "                                                        use_cache=False, \n",
    "                                                        device_map=\"auto\")\n",
    "tokenizer_llama_2_7b = AutoTokenizer.from_pretrained(model_llama_2_7b_id)\n",
    "model_llama_2_7b.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ff48c60-d38d-4ed1-83cf-17bc99366d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_llama_2_7b.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afba8ba9-6cbf-4ab8-a858-4187c7c1a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "            Answer this question.\n",
    "            \n",
    "            ### Question:\n",
    "            {sample['instruction']}\n",
    "\n",
    "            ### Answer:\n",
    "            {sample['response']}\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "999e8a86-f53f-4e92-8ed2-b4362028541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output dataset is ready for fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(sample, tokenizer):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "            Answer this question.\n",
    "            \n",
    "            ### Question:\n",
    "            {sample['instruction']}\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    sample['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    sample['labels'] = tokenizer(sample[\"response\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return sample\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    fn_kwargs={\"tokenizer\": tokenizer_llama_2_7b},\n",
    "    # batched=True\n",
    ")\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['instruction', 'response'])\n",
    "\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    tokenize_function, \n",
    "    fn_kwargs={\"tokenizer\": tokenizer_llama_2_7b},\n",
    "    # batched=True\n",
    ")\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(['instruction', 'response'])\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function, \n",
    "    fn_kwargs={\"tokenizer\": tokenizer_llama_2_7b},\n",
    "    # batched=True\n",
    ")\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['instruction', 'response'])\n",
    "\n",
    "print('The output dataset is ready for fine-tuning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad2c374-8b4e-4c9d-85ef-5aa324f2ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_llama_2_7b.config.pretraining_tp = 1\n",
    "\n",
    "# Validate that the model is using flash attention, by comparing doc strings\n",
    "if use_flash_attention:\n",
    "    from utils.llama_patch import forward\n",
    "    assert model_llama_2_7b.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n",
    "\n",
    "\n",
    "# tokenizer_llama_2_7b = AutoTokenizer.from_pretrained(model_llama_2_7b_id)\n",
    "tokenizer_llama_2_7b.pad_token = tokenizer_llama_2_7b.eos_token\n",
    "tokenizer_llama_2_7b.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d93ab-804b-44fb-aae7-688ffb2fdd29",
   "metadata": {},
   "source": [
    "The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad10396-3c56-4d25-b67e-e584ba880f61",
   "metadata": {},
   "source": [
    "Setup the PEFT/LoRA model for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d467c73b-a73a-4fdc-8887-774469ef2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921a7911-62e9-4c90-a053-96f786ec5bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 33554432\n",
      "all model parameters: 3533967360\n",
      "percentage of trainable model parameters: 0.95%\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", # Llama 2\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "model_llama_2_7b_peft = prepare_model_for_kbit_training(model_llama_2_7b)\n",
    "model_llama_2_7b_peft = get_peft_model(model_llama_2_7b_peft, peft_config)\n",
    "print(print_number_of_trainable_model_parameters(model_llama_2_7b_peft))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697f936-cff2-4bc0-9b44-0011f0b97142",
   "metadata": {},
   "source": [
    "### Train PEFT Adapter\n",
    "Define training arguments and create Trainer instance.\n",
    "\n",
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb305bf0-0f0d-4fff-9f40-9933033a35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"models/llama-7-int4-fine-tuned\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False # disable tqdm since with packing values are in correct\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b930587-788c-4ee4-b86c-97dc5a9595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_llama_2_7b_peft,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer_llama_2_7b,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=peft_training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9fb55b0-7d8a-4948-a131-26fc4e642784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "GPU 0:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 17115 MB\n",
      "  Used Memory: 5400 MB\n",
      "GPU Memory Used: 4.30 GB\n",
      "GPU Memory Cached: 4.49 GB\n"
     ]
    }
   ],
   "source": [
    "helper.check_gpu_capacity()\n",
    "# del model_llama_2_7b\n",
    "# del model_llama_2_7b_kbit\n",
    "helper.free_gpu_memory()\n",
    "helper.memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e1c7683-3fde-4d93-b3cf-5b1b474120ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 72/310 32:04 < 1:49:02, 0.04 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.700700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 23s, sys: 17min 4s, total: 32min 27s\n",
      "Wall time: 32min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbed1c-3988-4abd-bf3e-5866bb0ba801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "448b47bd-0af6-46eb-9843-18fcc8886da9",
   "metadata": {},
   "source": [
    "## Test Model and run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ae8839a-2449-4b22-8099-2ce24be47fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ef93b5cd4d4bd9913afc2256121064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_flash_attention:\n",
    "    # unpatch flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "peft_training_args.output_dir = \"models/llama-7-int4-fine-tuned\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_training_args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dd234-e0ec-4acb-88d5-532176e2b3ae",
   "metadata": {},
   "source": [
    "Let’s load the dataset again with a random sample to try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bae66a3a-e612-4299-a043-056193422b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated instruction:\n",
      "What are the main factors that cause lipid oxidation in food?\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# from random import randrange\n",
    "\n",
    "\n",
    "# # Load dataset from the hub and get a sample\n",
    "# dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "# sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "# prompt = f\"\"\"### Instruction:\n",
    "# Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "# ### Input:\n",
    "# {sample_ds['response']}\n",
    "\n",
    "# ### Response:\n",
    "# \"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "4.1.\tUV-Light\n",
    "Together with oxygen, UV-light can produce a singlet oxygen which can form directly a hydroperoxide from a unsaturated lipid (see Figure 3). Usually during the processing of powders, prevention of UV-light intrusion is being controlled. Furthermore, it depends on the package used, but as well how the consumer stores the product. \n",
    "4.2.\tOxygen\n",
    "Reaction with oxygen can take at various sites in the lipid oxidation chain reaction (see Figure 3). Exclusion of oxygen means mainly flushing inert gases typically nitrogen (or argon) to displace oxygen, packaging in vacuum or in modified atmosphere packaging (MAP). Oxygen scavengers can be employed to completely remove residual oxygen from the system. \n",
    "4.3.\tHeat\n",
    "Higher temperatures speed up the lipid oxidation reaction rates. Although the product should be prevented from heat, during production heat is needed to: \n",
    "1.\tbe able to handle ingredients (e.g. melting of fat, best done under nitrogen)\n",
    "2.\tget a product microbiologically stable\n",
    "3.\tspray dry the product\n",
    "Clearly heat is needed during production but where possible it should be minimized to prevent lipid oxidation. \n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.2)\n",
    "\n",
    "# print(f\"Prompt:\\n{sample_ds['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "# print(f\"Ground truth:\\n{sample_ds['instruction']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd5fe5-8b61-4fb8-baaf-aa8d1a85db0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8364c81d-11af-4ace-abe7-eb9e1f54a1ad",
   "metadata": {},
   "source": [
    "### Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 50 instructions and responses to save time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9366f8a9-d8f8-4656-ae8b-8e0c70a564b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'response'],\n",
       "    num_rows: 69\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c28e65-80a7-4a70-aa6b-f1aa48ac8d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is responsible for considering the nutritional losses during shelf life?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[41]['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e3938b6-3198-45d0-af70-a6234e0b6a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is responsible for considering the nutritional losses during shelf life?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The shelf life of the product is determined by the sensorial and nutritional parameters. The sensorial parameters are assessed by the sensorial panel and the nutritional parameters are assessed by the nutritional lab. The nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf life.\\n\\nThe nutritional lab is responsible for considering the nutritional losses during shelf'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = 41\n",
    "\n",
    "instruction = test_dataset[nn]['instruction']\n",
    "print(instruction)\n",
    "response = test_dataset[nn]['response']\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Answer this question.\n",
    "\n",
    "### Question:\n",
    "{instruction}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=400, \n",
    "                         do_sample=True, top_p=0.9, temperature=0.2)\n",
    "\n",
    "# print(f\"Prompt:\\n{sample_ds['response']}\\n\")\n",
    "generated_instruction = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "# generated_instruction = generated_instruction.split('\\n')[0].strip()\n",
    "generated_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bdff07a-0500-4c0c-af55-39ab54f7c7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'step 4. Shelf life Losses\\n\\n\\n\\nResponsible                                                                                                                                                                                                                                                                                                                                                                                                                  PD\\nThe 4th building block to set-up the nutritional range of a recipe is considering the nutritional losses during shelf life;                                                                                                                                                                                                                                                                                                  The 4th building block to set-up the nutritional range of a recipe is considering the nutritional losses during shelf life;'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[nn]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64a69243-f8ab-4846-988a-eb26ee48a8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The shelf life of the product is determined by the sensorial and nutritional '\n",
      " 'parameters. The sensorial parameters are assessed by the sensorial panel and '\n",
      " 'the nutritional parameters are assessed by the nutritional lab. The '\n",
      " 'nutritional lab is responsible for considering the nutritional losses during '\n",
      " 'shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf life.\\n'\n",
      " '\\n'\n",
      " 'The nutritional lab is responsible for considering the nutritional losses '\n",
      " 'during shelf')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(generated_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3eac5b1-bf5a-44c1-8db7-8e1c07b6c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# with torch.inference_mode():\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(f\"Prompt:\\n{sample_ds['response']}\\n\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m generated_instruction \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt):]\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/peft/peft_model.py:977\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/transformers/generation/utils.py:2642\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2639\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2642\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2650\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:420\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    419\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 420\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m(hidden_states)\n\u001b[1;32m    421\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    422\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/ai-project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "instruction_test = dataset['instruction'][50:100]\n",
    "response_test = dataset['response'][50:100]\n",
    "\n",
    "generated_instruction_list = []\n",
    "ground_truth_list = []\n",
    "\n",
    "for instruction, response in zip(instruction_test, response_test):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Answer this question.\n",
    "\n",
    "### Input:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    # with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.2)\n",
    "    \n",
    "    # print(f\"Prompt:\\n{sample_ds['response']}\\n\")\n",
    "    generated_instruction = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "    generated_instruction = generated_instruction.split('\\n')[0].strip()\n",
    "\n",
    "    generated_instruction_list.append(generated_instruction)\n",
    "    ground_truth_list.append(instruction)\n",
    "\n",
    "zipped_summaries = list(zip(generated_instruction_list, ground_truth_list))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['generated_instruction_list', 'ground_truth_list'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4dbe165-1095-424a-a0b7-e3785b705eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GI:  What is the difference between Viola toeria and Samphor?\n",
      "GT:  Identify which instrument is string or percussion: Samphor, Viola toeria \n",
      "\n",
      "GI:  What are the things found inside a house?\n",
      "GT:  If we were playing a game where we had to identify things that can be found inside a house, which of these would we call out: car, chair, table, park, cloud, microwave. \n",
      "\n",
      "GI:  What are the different types of beer?\n",
      "GT:  Classify the following as either dark-colored beers or light colored beers: porter, pilsner, stout, amber, lager \n",
      "\n",
      "GI:  \n",
      "GT:  Give me a bulleted list of the 5 highest mountains in the world and their respective heights in meters \n",
      "\n",
      "GI:  • When was Daniel Leavitt born?\n",
      "GT:  Extract all of the dates mentioned in this paragraph and list them using bullets in the format {Date} - {Description} \n",
      "\n",
      "GI:  What products does Apple sell?\n",
      "GT:  Which products apple sell? \n",
      "\n",
      "GI:  What is the moral of the story?\n",
      "GT:  Write a short story about a person who discovers a hidden room in their house. The story should include a plot twist and a clear resolution at the end. \n",
      "\n",
      "GI:  What is oil used for?\n",
      "GT:  Please describe  what is oil and give me a list of it’s applications. \n",
      "\n",
      "GI:  What are the different types of rain?\n",
      "GT:  List 6 different types of rain in Seattle \n",
      "\n",
      "GI:  What is investment banking?\n",
      "GT:  What is investment banking? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in df[40:50].iterrows():\n",
    "    print('GI: ', row['generated_instruction_list'])\n",
    "\n",
    "    print('GT: ',row['ground_truth_list'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18e69970-5f90-4a9a-926b-92ad420a8b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate==0.4.0 rouge_score==0.1.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ce941-84b9-4b29-8f27-55e71b5ef0cf",
   "metadata": {},
   "source": [
    "Compute ROUGE score for this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "161c4c35-8bc6-491d-a9af-8a7b4aab0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>0.467613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>0.297052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>0.436774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>0.434922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0     rouge1  0.467613\n",
       "1     rouge2  0.297052\n",
       "2     rougeL  0.436774\n",
       "3  rougeLsum  0.434922"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "model_results = rouge.compute(\n",
    "    predictions=generated_instruction_list,\n",
    "    references=ground_truth_list[0:len(generated_instruction_list)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('MODEL:')\n",
    "model_results_df = pd.DataFrame(model_results.items(), columns=['Metric', 'Value'])\n",
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902ce2c-3b32-43f5-b900-9260254c9d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21c03f-3b77-42c7-aa82-d3b95fc1b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1f91d-826f-4f7b-8d8f-a6e22acbe45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae1f0f-e293-4526-93da-ed10e1a7da0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f79f6-249d-4426-9fbf-4e194e404370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d9165-9add-4f45-a891-11874d773931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274672-da07-45e6-8efc-34fa1d83f6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818dfa0-18d1-4bb2-9832-5bfd6542b0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb86107-0fd9-4afe-bd97-1c2797ea871d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe148763-7e03-4483-95a8-27f3d54b9ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5606d0-7762-435e-8dca-2c0b07fbf731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
