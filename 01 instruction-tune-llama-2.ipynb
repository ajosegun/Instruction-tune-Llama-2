{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053535bf-73df-4b63-9c84-356c701136ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d5c91-e135-4452-960e-8e37aa4fa54d",
   "metadata": {},
   "source": [
    "#### fix for could not import -lmza\n",
    "https://github.com/lucidrains/imagen-pytorch/issues/92 \n",
    "\n",
    "#### Fix for Can't import datasets AttributeError: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)\n",
    "\n",
    "Restart juypyter and kernel\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921b8a86-e2c6-40fa-9e28-a597eb2bf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f87d5a5-e6ae-4234-914a-5f294b387b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "GPU 0:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n",
      "GPU 1:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n",
      "GPU 2:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n",
      "GPU 3:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import time\n",
    "helper.check_gpu_capacity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d9cd78-87e0-4456-81f7-e94932af0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb913e1-0e58-4a35-8630-8e87fd15cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> instruction\n",
      "====================\n",
      "When is the World Teacher's Day celebrated?\n",
      "====================\n",
      "--> context\n",
      "====================\n",
      "\n",
      "====================\n",
      "--> response\n",
      "====================\n",
      "World Teacher's Day is celebrated on 5th October\n",
      "====================\n",
      "--> category\n",
      "====================\n",
      "open_qa\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "sample_ds = dataset[randrange(len(dataset))]\n",
    "\n",
    "for key, value in sample_ds.items():\n",
    "    print(f\"--> {key}\")\n",
    "    print(\"====================\")\n",
    "    print(value)\n",
    "    print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fe73ef-bd68-429a-8adb-080314586b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brainstorming',\n",
       " 'classification',\n",
       " 'closed_qa',\n",
       " 'creative_writing',\n",
       " 'general_qa',\n",
       " 'information_extraction',\n",
       " 'open_qa',\n",
       " 'summarization'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3529697a-f1d5-410e-a813-23fee88ba9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset.map( ['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48905876-1a02-40cb-b4c6-1e8f09b8130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(filter(lambda x :x != '', list(map(lambda x:((x['category'], len(x['context'])) if len(x['context']) > 5000 else ''), dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde2f4f-f086-4e18-90ca-6871834b9a51",
   "metadata": {},
   "source": [
    "Based on our use case, we want to focus on 'general_qa' and  'information_extraction tasks.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e70e37-693e-4541-9d5c-9c36391d9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-67fe253a0190166e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summarization', 'closed_qa', 'information_extraction'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 4467\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_categories = ['closed_qa', 'summarization', 'information_extraction']\n",
    "filtered_dataset = dataset.filter(lambda x: x['category'] in target_categories)\n",
    "\n",
    "print(set(filtered_dataset['category']))\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d8c2c9-415b-4214-b2fd-81e4f489df25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> instruction\n",
      "====================\n",
      "Given this paragraph, what key crops were not available in Europe before the colonization of the New World?\n",
      "====================\n",
      "--> context\n",
      "====================\n",
      "Food historian Lois Ellen Frank calls potatoes, tomatoes, corn, beans, squash, chili, cacao, and vanilla the \"magic eight\" ingredients that were found and used only in the Americas before 1492 and were taken via the Columbian Exchange back to the Old World, dramatically transforming the cuisine there. According to Frank,\n",
      "If we deconstruct that these foods were inherently native, then that means that the Italians didn't have the tomato, the Irish didn't have the potato, half the British National Dish—Fish and Chips—didn't exist. The Russians didn't have the potato, nor did they have vodka from the potato. There were no chiles in any Asian cuisine anywhere in the world, nor were there any chiles in any East Indian cuisine dishes, including curries. And the French had no confection using either vanilla or chocolate. So the Old World was a completely different place.\n",
      "====================\n",
      "--> response\n",
      "====================\n",
      "Potatoes, tomatoes, corn, beans, squash, chili, cacao, and vanilla were not available in the Old World prior to the European discovery of the Americas\n",
      "====================\n",
      "--> category\n",
      "====================\n",
      "closed_qa\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "sample_ds = filtered_dataset[randrange(len(filtered_dataset))]\n",
    "\n",
    "for key, value in sample_ds.items():\n",
    "    print(f\"--> {key}\")\n",
    "    print(\"====================\")\n",
    "    print(value)\n",
    "    print(\"====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fbfd21-dd46-4044-95d1-f91d4f3374cb",
   "metadata": {},
   "source": [
    "To instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a formatting_function that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8ff2b8-d4e0-40af-ad22-06442d57a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_instruction(sample):\n",
    "#     return f\"\"\"### Instruction:\n",
    "#             Use the Input and context below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "            \n",
    "#             ### Input:\n",
    "#             {sample['response']}\n",
    "    \n",
    "#             ### Context:\n",
    "#             {sample['context'] if sample['context'] != '' else ''}\n",
    "    \n",
    "#             ### Response:\n",
    "#             {sample['instruction']}\n",
    "#         \"\"\"\n",
    "\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "            Use the Input and context below to create an instruction, which could have been used to generate the input using an LLM.             \n",
    "            {sample['context'] if sample['context'] != '' else ''}\n",
    "            \n",
    "            {sample['response']}\n",
    "\n",
    "            ### Response:\n",
    "            {sample['instruction']}\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25d8e6-e589-48a3-9aee-6b03d66389c4",
   "metadata": {},
   "source": [
    "Let's test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e89814-1a71-474c-babc-9c398972125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "            Use the Input and context below to create an instruction, which could have been used to generate the input using an LLM.             \n",
      "            Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the \"Thunderer of the Nile\", and described them as the \"protectors\" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by electric catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients with ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them.\n",
      "Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing.\n",
      "Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.\n",
      "Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete, in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word electricus (\"of amber\" or \"like amber\",, elektron, the Greek word for \"amber\") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words \"electric\" and \"electricity\", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.\n",
      "Further work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges\n",
      "In 1775, Hugh Williamson reported a series of experiments to the Royal Society on the shocks delivered by the electric eel; that same year the surgeon and anatomist John Hunter described the structure of the fish's electric organs. In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his \"On Physical Lines of Force\" in 1861 and 1862. \n",
      "While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life.\n",
      "In 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for \"his discovery of the law of the photoelectric effect\". The photoelectric effect is also employed in photocells such as can be found in solar panels.\n",
      "The first solid-state device was the \"cat's-whisker detector\" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.\n",
      "Solid-state electronics came into its own with the emergence of transistor technology. The first working transistor, a germanium-based point-contact transistor, was invented by John Bardeen and Walter Houser Brattain at Bell Labs in 1947, followed by the bipolar junction transistor in 1948.\n",
      "            \n",
      "            Further work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges\n",
      "In 1775, Hugh Williamson reported a series of experiments to the Royal Society on the shocks delivered by the electric eel; that same year the surgeon and anatomist John Hunter described the structure of the fish's electric organs. In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used.\n",
      "\n",
      "            ### Response:\n",
      "            Extract from the following passage research related to electricity in the 17th and 18th centuries.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_instruction(filtered_dataset[randrange(len(filtered_dataset))]))\n",
    "\n",
    "## Do I need to add the context in the format_instruction?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9519a9-759d-4a77-92f7-c7f4b8534bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "            Use the Input and context below to create an instruction, which could have been used to generate the input using an LLM.             \n",
      "            Food historian Lois Ellen Frank calls potatoes, tomatoes, corn, beans, squash, chili, cacao, and vanilla the \"magic eight\" ingredients that were found and used only in the Americas before 1492 and were taken via the Columbian Exchange back to the Old World, dramatically transforming the cuisine there. According to Frank,\n",
      "If we deconstruct that these foods were inherently native, then that means that the Italians didn't have the tomato, the Irish didn't have the potato, half the British National Dish—Fish and Chips—didn't exist. The Russians didn't have the potato, nor did they have vodka from the potato. There were no chiles in any Asian cuisine anywhere in the world, nor were there any chiles in any East Indian cuisine dishes, including curries. And the French had no confection using either vanilla or chocolate. So the Old World was a completely different place.\n",
      "            \n",
      "            Potatoes, tomatoes, corn, beans, squash, chili, cacao, and vanilla were not available in the Old World prior to the European discovery of the Americas\n",
      "\n",
      "            ### Response:\n",
      "            Given this paragraph, what key crops were not available in Europe before the colonization of the New World?\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(format_instruction(sample_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7190fda-5936-4163-910a-c1a8e7e8b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-be8fe28074092229.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 15011\n",
      "Train size: 3216\n",
      "Eval size: 804\n",
      "Test size: 447\n"
     ]
    }
   ],
   "source": [
    "shuffled_dataset = filtered_dataset.shuffle(seed=42)\n",
    "shuffled_dataset = shuffled_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = shuffled_dataset['train']\n",
    "test_dataset = shuffled_dataset['test']\n",
    "\n",
    "train_eval_dataset = train_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_eval_dataset['train']\n",
    "eval_dataset = train_eval_dataset['test']\n",
    "\n",
    "\n",
    "# Print the sizes of the train, eval, andtest sets\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0156c-85de-4e8a-aeb0-8f7f8d785212",
   "metadata": {},
   "source": [
    "### Instruction-tune Llama 2 using trl and the SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a914be7-846c-4196-be7f-7a526ad89739",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec40fdb-1621-42a2-901c-71b8a287d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "GPU 0:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n",
      "GPU 1:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n",
      "GPU 2:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n",
      "GPU 3:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 22511 MB\n",
      "  Used Memory: 4 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "helper.check_gpu_capacity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf02c98-762b-4fc6-89ec-0fd1c7741cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d29288-3392-4e22-952b-d39d271af9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793594d5ccdc4f9284ba00724fd0d156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aedea91a84f406180baaacb5dc87a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8439f627984f319dc489d3decac88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078d8541d5654e52900afc1718323a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d01dce98db4ddb93b6ac0b31b2a68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066cbb62d8174ddd8e025052db040869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2620f6da8c417f920cc1f9a51eb61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a255ab9778438ab7c0ef2ec9a61ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7771073b5bc94a88a85ff778b18b7398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b30491862947418cb99cce30d83b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eda0f3191c45d7bb78734f387287bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hugging Face model id\n",
    "model_llama_2_7b_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# torch.cuda.set_device(1)\n",
    "# Load model and tokenizer\n",
    "model_llama_2_7b = AutoModelForCausalLM.from_pretrained(model_llama_2_7b_id, \n",
    "                                                        quantization_config=bnb_config, \n",
    "                                                        use_cache=False, \n",
    "                                                        device_map=\"auto\")\n",
    "# model_llama_2_7b.to_bettertransformer()\n",
    "tokenizer_llama_2_7b = AutoTokenizer.from_pretrained(model_llama_2_7b_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f072c-398d-4c55-ae4a-ebaa8994b56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16f0ba-9f6a-457e-a279-1d91e8fdd0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_ds = filtered_dataset[35]\n",
    "\n",
    "sample_ds = filtered_dataset[randrange(len(filtered_dataset))]\n",
    "\n",
    "# prompt = f\"\"\"### Instruction:\n",
    "#             Use the Input and context below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "            \n",
    "#             ### Input:\n",
    "#             {sample_ds['response']}\n",
    "\n",
    "#             ### Context:\n",
    "#             {sample_ds['context'] if sample_ds['context'] != '' else ''}\n",
    "\n",
    "#             ### Response:\n",
    "#         \"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM.             \n",
    "            {sample_ds['context']}\n",
    "            \n",
    "            {sample_ds['response']}\n",
    "\n",
    "            ### Response:\n",
    "        \"\"\"\n",
    "\n",
    "input_ids = tokenizer_llama_2_7b(prompt, return_tensors=\"pt\", \n",
    "                                 # max_length=2000,\n",
    "                                 truncation=True\n",
    "                                ).input_ids.to('cuda')\n",
    "# with torch.inference_mode():\n",
    "outputs = model_llama_2_7b.generate(input_ids=input_ids, max_new_tokens=200, \n",
    "                                    do_sample=True, top_p=0.9, temperature=0.9\n",
    "                                   )\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer_llama_2_7b.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample_ds['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00a528a0-bcc7-4186-9a04-b72ad68f5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From above, we see that the generated response is not okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cad2c374-8b4e-4c9d-85ef-5aa324f2ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_llama_2_7b.config.pretraining_tp = 1\n",
    "\n",
    "# tokenizer_llama_2_7b = AutoTokenizer.from_pretrained(model_llama_2_7b_id)\n",
    "tokenizer_llama_2_7b.pad_token = tokenizer_llama_2_7b.eos_token\n",
    "tokenizer_llama_2_7b.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d93ab-804b-44fb-aae7-688ffb2fdd29",
   "metadata": {},
   "source": [
    "The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e355c44-b1c0-4755-b316-a59e2a93fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "921a7911-62e9-4c90-a053-96f786ec5bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 3,533,967,360 || trainable%: 0.9494833591219133\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "model_llama_2_7b = prepare_model_for_kbit_training(model_llama_2_7b)\n",
    "model_llama_2_7b = get_peft_model(model_llama_2_7b, peft_config)\n",
    "# print(print_number_of_trainable_model_parameters(model_llama_2_7b_peft))\n",
    "model_llama_2_7b.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697f936-cff2-4bc0-9b44-0011f0b97142",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb305bf0-0f0d-4fff-9f40-9933033a35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"models/peft-llama-7-int4-instruct-generation-training\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    # save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False, # disable tqdm since with packing values are in correct\n",
    "    save_total_limit = 2,\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "438028ae-1eb9-4ac6-a33e-fd78e06d899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='NousResearch/Llama-2-7b-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=['q_proj', 'v_proj'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b930587-788c-4ee4-b86c-97dc5a9595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "# response_template = \" ### Response:\"\n",
    "# collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama_2_7b)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_llama_2_7b,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer_llama_2_7b,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    "    # data_collator=collator,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9fb55b0-7d8a-4948-a131-26fc4e642784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "GPU 0:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 20511 MB\n",
      "  Used Memory: 2004 MB\n",
      "GPU 1:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 20813 MB\n",
      "  Used Memory: 1702 MB\n",
      "GPU 2:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 20813 MB\n",
      "  Used Memory: 1702 MB\n",
      "GPU 3:\n",
      "  Total Memory: 23028 MB\n",
      "  Free Memory: 20511 MB\n",
      "  Used Memory: 2004 MB\n",
      "GPU Memory Used: 1.12 GB\n",
      "GPU Memory Cached: 1.20 GB\n"
     ]
    }
   ],
   "source": [
    "helper.check_gpu_capacity()\n",
    "# del model_llama_2_7b\n",
    "# del model_llama_2_7b_kbit\n",
    "helper.free_gpu_memory()\n",
    "helper.memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c7683-3fde-4d93-b3cf-5b1b474120ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='1206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  15/1206 06:06 < 9:18:54, 0.04 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.521900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "# After training, access the path of the best checkpoint like this\n",
    "best_ckpt_path = trainer.state.best_model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbed1c-3988-4abd-bf3e-5866bb0ba801",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b47bd-0af6-46eb-9843-18fcc8886da9",
   "metadata": {},
   "source": [
    "## Test Model and run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ae8839a-2449-4b22-8099-2ce24be47fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c11f383f7934d099167a5fbbaf88eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_flash_attention:\n",
    "    # unpatch flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# args.output_dir = \"llama-7-int4-dolly-filtered\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dd234-e0ec-4acb-88d5-532176e2b3ae",
   "metadata": {},
   "source": [
    "Let’s load the dataset again with a random sample to try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bae66a3a-e612-4299-a043-056193422b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated instruction:\n",
      "What are the main factors that influence lipid oxidation in food?\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# from random import randrange\n",
    "\n",
    "\n",
    "# # Load dataset from the hub and get a sample\n",
    "# dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "# sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "# prompt = f\"\"\"### Instruction:\n",
    "# Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "# ### Input:\n",
    "# {sample_ds['response']}\n",
    "\n",
    "# ### Response:\n",
    "# \"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "4.1.\tUV-Light\n",
    "Together with oxygen, UV-light can produce a singlet oxygen which can form directly a hydroperoxide from a unsaturated lipid (see Figure 3). Usually during the processing of powders, prevention of UV-light intrusion is being controlled. Furthermore, it depends on the package used, but as well how the consumer stores the product. \n",
    "4.2.\tOxygen\n",
    "Reaction with oxygen can take at various sites in the lipid oxidation chain reaction (see Figure 3). Exclusion of oxygen means mainly flushing inert gases typically nitrogen (or argon) to displace oxygen, packaging in vacuum or in modified atmosphere packaging (MAP). Oxygen scavengers can be employed to completely remove residual oxygen from the system. \n",
    "4.3.\tHeat\n",
    "Higher temperatures speed up the lipid oxidation reaction rates. Although the product should be prevented from heat, during production heat is needed to: \n",
    "1.\tbe able to handle ingredients (e.g. melting of fat, best done under nitrogen)\n",
    "2.\tget a product microbiologically stable\n",
    "3.\tspray dry the product\n",
    "Clearly heat is needed during production but where possible it should be minimized to prevent lipid oxidation. \n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.2)\n",
    "\n",
    "# print(f\"Prompt:\\n{sample_ds['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "# print(f\"Ground truth:\\n{sample_ds['instruction']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "61dd5fe5-8b61-4fb8-baaf-aa8d1a85db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated instruction:\n",
      "\n",
      "            ### Context:\n",
      "            This is a list of sovereign states and dependent territories in Africa. It includes both fully recognised states, states with limited or zero recognition, and dependent territories of both African and non-African states. It lists 56 sovereign states (54 of which are member states of the United Nations), two non-sovereign (dependent) territories of non-African sovereign states, and ten sub-national regions of non-African sovereign states. Malta and parts of France, Italy, Portugal, and Spain are located on the African continental plate, some considerably closer to the African mainland than the European mainland but, politically, are generally considered to be European by convention. Egypt, although extending into Asia through the Sinai Peninsula, is considered an African state.\n",
      "    \n",
      "            ### Response:\n",
      "            How many sovereign\n",
      "Ground truth:\n",
      "What is the total number of geographical entities (states, territories, regions) in Africa?\n"
     ]
    }
   ],
   "source": [
    "sample_ds = filtered_dataset[randrange(len(filtered_dataset))]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "       \n",
    "            {sample_ds['response']}\n",
    "            {sample_ds['context'] if sample_ds['context'] != '' else ''}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", \n",
    "                                 # max_length=4000,\n",
    "                                 truncation=True\n",
    "                                ).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=200, \n",
    "                        do_sample=True, \n",
    "                         top_p=0.8, \n",
    "                         temperature=0.9\n",
    "                                   )\n",
    "\n",
    "# print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample_ds['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c110286-36a6-4cf2-bcef-499bca223aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8364c81d-11af-4ace-abe7-eb9e1f54a1ad",
   "metadata": {},
   "source": [
    "### Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 50 instructions and responses to save time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9366f8a9-d8f8-4656-ae8b-8e0c70a564b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 15011\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a38a2df2-31a6-4309-ba03-25decfe6f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction\n",
      "[\"Give me a bulleted list of Aaron Fenster's accomplishments.\", 'Did Thomas Attewell bat with a right or left hand?']\n",
      "context\n",
      "['Aaron Fenster is a medical physicist at the University of Western Ontario Robarts Research Institute in London, Ontario, Canada. He was named a Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2013 for his contributions to medical imaging and three-dimensional ultrasound-guided interventions. He is also a fellow of the Canadian Academy of Health Sciences and co-program director of the Ontario Institute for Cancer Research Imaging Program. He holds Ph.D. from the University of Toronto and received further training at the Ontario Cancer Institute.', 'Thomas Attewell (7 November 1869 – 6 July 1937) was an English first-class cricketer. Attewell was a right-handed batsman who bowled right-arm medium pace. He was born at Keyworth, Nottinghamshire.\\n\\nHis brother William played first-class cricket for Nottinghamshire and the Marylebone Cricket Club, as well as Test cricket for England. His cousin Walter Attewell played first-class cricket for Nottinghamshire.']\n",
      "response\n",
      "['- Medical physicist at the University of Western Ontario Robarts Research Institute.\\n- Fellow of the Institute of Electrical and Electronics Engineers (IEEE).\\n- Fellow of the Canadian Academy of Health Sciences. \\n- Co-program director of the Ontario Institute for Cancer Research Imaging Program. \\n- Completed a Ph.D. from the University of Toronto.', 'right hand']\n",
      "category\n",
      "['information_extraction', 'closed_qa']\n"
     ]
    }
   ],
   "source": [
    "for key, value in test_dataset[50:52].items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f3eac5b1-bf5a-44c1-8db7-8e1c07b6c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_instruction_list</th>\n",
       "      <th>ground_truth_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>Give me a bulleted list of Aaron Fenster's acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n            ### Input:\\n            His brot...</td>\n",
       "      <td>Did Thomas Attewell bat with a right or left h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n            ### Context:\\n            Honolu...</td>\n",
       "      <td>What is the capital of Hawaii?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n            ### Input:\\n            An AI sy...</td>\n",
       "      <td>What is the main prerequisite for an AI to sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n            ### Context:\\n            Artifi...</td>\n",
       "      <td>What are artificial neural networks?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n            ### Context:\\n            \"Colou...</td>\n",
       "      <td>What is Color of the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n            ### Input:\\n            Eggs, ha...</td>\n",
       "      <td>Extract the ingredients needed to make pasta c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>Who was the best team in the NFL during the 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n            ### Response:\\n            Who w...</td>\n",
       "      <td>Who was the first African American to go to sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n            ### Input:\\n            Haakon, ...</td>\n",
       "      <td>Give me a quick brief of the Prince of Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\n            ### Response:\\n            When ...</td>\n",
       "      <td>Given this text about gradient descent, what i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>Given the reference text about Tottenham Hotsp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\n            ### Response:\\n            Why w...</td>\n",
       "      <td>What enabled Libya to obtain rapid economic gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\n            ### Context:\\n            Sex an...</td>\n",
       "      <td>What is \"Sex and the City\"?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\n            ### Context:\\n            Simón ...</td>\n",
       "      <td>Given this paragraph about Simon Bolivar, tell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\\n            ### Response:\\n            Where...</td>\n",
       "      <td>Where is the Stade Louis-ll located?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\n            ### Context:\\n            Ukrain...</td>\n",
       "      <td>Using the two paragraphs below, when was the U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\n            Please give me a bulleted list o...</td>\n",
       "      <td>What are the four major subregions in Central ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\n            ### Response:\\n            Who i...</td>\n",
       "      <td>Who are the people named in the passage?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\n            ### Context:\\n            The Ad...</td>\n",
       "      <td>What is Adventures of Tintin?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\\n            ### Input:\\n            Sci-fi f...</td>\n",
       "      <td>What is Sci-fi fantasy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>Who are el Marinid ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\\n            ### Context:\\n            Adams,...</td>\n",
       "      <td>Tell me about Clarence Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2004: Twenty20 cricket\\nTendulkar was one of t...</td>\n",
       "      <td>How many runs did Sachin score in the 2011 wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>What was written in Ski-U-Mah?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\\n            ### Context:\\n            Anton ...</td>\n",
       "      <td>Given this paragraph on Anton Chekhov, please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\\n            ### Response:\\n            Who i...</td>\n",
       "      <td>Who is Bon Jovi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>summarize the responsibilities of a data archi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>Given a reference text about the adverse affec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\\n            ### Response:\\n            Based...</td>\n",
       "      <td>From the Passage please find out what are the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\\n            ### Input:\\n            The gold...</td>\n",
       "      <td>When did the golden age for Jewish culture sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>\\n            ### Context:\\n            Hallma...</td>\n",
       "      <td>From the passage provided, extract when hallma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>\\n            ### Response:\\n            Was V...</td>\n",
       "      <td>Was Michael Flavin wealthy at the time he wrot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>\\n            ### Input:\\n            The Worl...</td>\n",
       "      <td>From the passage note down the name of the cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>\\n            ### Input:\\n            The foll...</td>\n",
       "      <td>Which moons of Saturn did Pioneer 11 encounter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>\\n            ### Input:\\n            Yinyuan ...</td>\n",
       "      <td>per this reference of Itsunen Shoyu, who did h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>\\n            ### Context:\\n            The 20...</td>\n",
       "      <td>Based on the information provided, what was th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>\\n            ### Response:\\n            Who a...</td>\n",
       "      <td>List all the names of people mentioned in this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>\\n            ### Context:\\n            Crashm...</td>\n",
       "      <td>Using given text as a reference, write some de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>\\n            ### Response:\\n            When ...</td>\n",
       "      <td>Why does Nadal play tennis left-handed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>\\n            ### Context:\\n            Aliasi...</td>\n",
       "      <td>Please give me an example of this phenomenon t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>\\n            ### Response:\\n            Which...</td>\n",
       "      <td>What countries have won the rugby world cup?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>\\n            ### Response:\\n            Who i...</td>\n",
       "      <td>Who is Carl Rimmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>\\n            ### Input:\\n            Given th...</td>\n",
       "      <td>What therapies are available for Autism?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>\\n            ### Context:\\n            Hair o...</td>\n",
       "      <td>Extract out some details about Hair of the Dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>\\n            ### Response:\\n            Is de...</td>\n",
       "      <td>Do you think depression ran in Wittgenstein's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>\\n            ### Input:\\n            What is ...</td>\n",
       "      <td>When did mountain biking start?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>\\n            ### Input:\\n            Who is R...</td>\n",
       "      <td>Given this paragraph, explain who Robert Kiyos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>\\n            ### Response:\\n            What ...</td>\n",
       "      <td>Tell me about the 1999 UEFA champions league f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>\\n            ### Input:\\n            Mark Arm...</td>\n",
       "      <td>Given this paragraph about Mark Arminski, why ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           generated_instruction_list  \\\n",
       "0   \\n            ### Response:\\n            What ...   \n",
       "1   \\n            ### Input:\\n            His brot...   \n",
       "2   \\n            ### Context:\\n            Honolu...   \n",
       "3   \\n            ### Input:\\n            An AI sy...   \n",
       "4   \\n            ### Context:\\n            Artifi...   \n",
       "5   \\n            ### Context:\\n            \"Colou...   \n",
       "6   \\n            ### Input:\\n            Eggs, ha...   \n",
       "7   \\n            ### Response:\\n            What ...   \n",
       "8   \\n            ### Response:\\n            Who w...   \n",
       "9   \\n            ### Input:\\n            Haakon, ...   \n",
       "10  \\n            ### Response:\\n            When ...   \n",
       "11  \\n            ### Response:\\n            What ...   \n",
       "12  \\n            ### Response:\\n            Why w...   \n",
       "13  \\n            ### Context:\\n            Sex an...   \n",
       "14  \\n            ### Context:\\n            Simón ...   \n",
       "15  \\n            ### Response:\\n            Where...   \n",
       "16  \\n            ### Context:\\n            Ukrain...   \n",
       "17  \\n            Please give me a bulleted list o...   \n",
       "18  \\n            ### Response:\\n            Who i...   \n",
       "19  \\n            ### Context:\\n            The Ad...   \n",
       "20  \\n            ### Input:\\n            Sci-fi f...   \n",
       "21  \\n            ### Response:\\n            What ...   \n",
       "22  \\n            ### Context:\\n            Adams,...   \n",
       "23  2004: Twenty20 cricket\\nTendulkar was one of t...   \n",
       "24  \\n            ### Response:\\n            What ...   \n",
       "25  \\n            ### Context:\\n            Anton ...   \n",
       "26  \\n            ### Response:\\n            Who i...   \n",
       "27  \\n            ### Response:\\n            What ...   \n",
       "28  \\n            ### Response:\\n            What ...   \n",
       "29  \\n            ### Response:\\n            Based...   \n",
       "30  \\n            ### Input:\\n            The gold...   \n",
       "31  \\n            ### Context:\\n            Hallma...   \n",
       "32  \\n            ### Response:\\n            Was V...   \n",
       "33  \\n            ### Input:\\n            The Worl...   \n",
       "34  \\n            ### Input:\\n            The foll...   \n",
       "35  \\n            ### Input:\\n            Yinyuan ...   \n",
       "36  \\n            ### Context:\\n            The 20...   \n",
       "37  \\n            ### Response:\\n            Who a...   \n",
       "38  \\n            ### Context:\\n            Crashm...   \n",
       "39  \\n            ### Response:\\n            When ...   \n",
       "40  \\n            ### Context:\\n            Aliasi...   \n",
       "41  \\n            ### Response:\\n            Which...   \n",
       "42  \\n            ### Response:\\n            Who i...   \n",
       "43  \\n            ### Input:\\n            Given th...   \n",
       "44  \\n            ### Context:\\n            Hair o...   \n",
       "45  \\n            ### Response:\\n            Is de...   \n",
       "46  \\n            ### Input:\\n            What is ...   \n",
       "47  \\n            ### Input:\\n            Who is R...   \n",
       "48  \\n            ### Response:\\n            What ...   \n",
       "49  \\n            ### Input:\\n            Mark Arm...   \n",
       "\n",
       "                                    ground_truth_list  \n",
       "0   Give me a bulleted list of Aaron Fenster's acc...  \n",
       "1   Did Thomas Attewell bat with a right or left h...  \n",
       "2                      What is the capital of Hawaii?  \n",
       "3   What is the main prerequisite for an AI to sur...  \n",
       "4                What are artificial neural networks?  \n",
       "5                          What is Color of the world  \n",
       "6   Extract the ingredients needed to make pasta c...  \n",
       "7   Who was the best team in the NFL during the 19...  \n",
       "8   Who was the first African American to go to sp...  \n",
       "9       Give me a quick brief of the Prince of Norway  \n",
       "10  Given this text about gradient descent, what i...  \n",
       "11  Given the reference text about Tottenham Hotsp...  \n",
       "12  What enabled Libya to obtain rapid economic gr...  \n",
       "13                        What is \"Sex and the City\"?  \n",
       "14  Given this paragraph about Simon Bolivar, tell...  \n",
       "15               Where is the Stade Louis-ll located?  \n",
       "16  Using the two paragraphs below, when was the U...  \n",
       "17  What are the four major subregions in Central ...  \n",
       "18           Who are the people named in the passage?  \n",
       "19                      What is Adventures of Tintin?  \n",
       "20                            What is Sci-fi fantasy?  \n",
       "21                               Who are el Marinid ?  \n",
       "22                       Tell me about Clarence Adams  \n",
       "23  How many runs did Sachin score in the 2011 wor...  \n",
       "24                     What was written in Ski-U-Mah?  \n",
       "25  Given this paragraph on Anton Chekhov, please ...  \n",
       "26                                    Who is Bon Jovi  \n",
       "27  summarize the responsibilities of a data archi...  \n",
       "28  Given a reference text about the adverse affec...  \n",
       "29  From the Passage please find out what are the ...  \n",
       "30  When did the golden age for Jewish culture sta...  \n",
       "31  From the passage provided, extract when hallma...  \n",
       "32  Was Michael Flavin wealthy at the time he wrot...  \n",
       "33  From the passage note down the name of the cou...  \n",
       "34  Which moons of Saturn did Pioneer 11 encounter...  \n",
       "35  per this reference of Itsunen Shoyu, who did h...  \n",
       "36  Based on the information provided, what was th...  \n",
       "37  List all the names of people mentioned in this...  \n",
       "38  Using given text as a reference, write some de...  \n",
       "39            Why does Nadal play tennis left-handed?  \n",
       "40  Please give me an example of this phenomenon t...  \n",
       "41       What countries have won the rugby world cup?  \n",
       "42                                 Who is Carl Rimmer  \n",
       "43           What therapies are available for Autism?  \n",
       "44  Extract out some details about Hair of the Dog...  \n",
       "45  Do you think depression ran in Wittgenstein's ...  \n",
       "46                    When did mountain biking start?  \n",
       "47  Given this paragraph, explain who Robert Kiyos...  \n",
       "48  Tell me about the 1999 UEFA champions league f...  \n",
       "49  Given this paragraph about Mark Arminski, why ...  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "instruction_test = test_dataset['instruction'][50:100]\n",
    "response_test = test_dataset['response'][50:100]\n",
    "context_test = test_dataset['context'][50:100]\n",
    "\n",
    "\n",
    "generated_instruction_list = []\n",
    "ground_truth_list = []\n",
    "\n",
    "for instruction, response, context in zip(instruction_test, response_test, context_test):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "            \n",
    "            {response}\n",
    "            {context if context != '' else ''}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", \n",
    "                                     # max_length=4000,\n",
    "                                     truncation=True\n",
    "                                    ).input_ids.cuda()\n",
    "    # with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=200, \n",
    "                            do_sample=True, \n",
    "                             top_p=0.8, \n",
    "                             temperature=0.9\n",
    "                                       )\n",
    "    \n",
    "    # print(f\"Prompt:\\n{sample_ds['response']}\\n\")\n",
    "    generated_instruction = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "    # generated_instruction = generated_instruction.split('\\n')[0].strip()\n",
    "\n",
    "    generated_instruction_list.append(generated_instruction)\n",
    "    ground_truth_list.append(instruction)\n",
    "\n",
    "zipped_summaries = list(zip(generated_instruction_list, ground_truth_list))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['generated_instruction_list', 'ground_truth_list'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c4dbe165-1095-424a-a0b7-e3785b705eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GI:  \n",
      "            ### Response:\n",
      "            What are some notable achievements of Aaron Fenster?\n",
      "        \n",
      "GT:  Give me a bulleted list of Aaron Fenster's accomplishments. \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            His brother William played first-class cricket for Nottinghamshire and the Marylebone Cricket Club, as well as Test cricket for England.\n",
      "    \n",
      "            ### Context:\n",
      "            William Attewell (25 May 1863 – 25 May 1932) was an English first-class cricketer. Attewell was a right-handed batsman who bowled right-arm medium pace. He was born at Keyworth, Nottinghamshire.\n",
      "\n",
      "His brother Thomas played first-class cricket for Nottinghamshire and the Marylebone Cricket Club, as well as Test cricket for England. His cousin Walter Attewell played first-class cricket for Nottinghamshire.\n",
      "    \n",
      "            ### Response:\n",
      "            Who played Test cricket for England?\n",
      "        \n",
      "GT:  Did Thomas Attewell bat with a right or left hand? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Honolulu (/ˌhɒnəˈluːli/ HON-ə-loo-lee; Hawaiian: [honoˈlulu] listen (help·info)) is the capital and most populous city of the U.S. state of Hawaii. With a 2020 population of 390,738, Honolulu is the thirteenth-most populous city in the United States. Honolulu is the principal city of the Hawaiian Islands, which are an unincorporated territory of the United States. According to the U.S. Census Bureau, Honolulu is the most ethnically diverse major city in the United States. The city is the westernmost and southernmost major U.S. city and the major population center of the islands of Hawaii.\n",
      "\n",
      "Honolulu is situated on the south-central part of the island\n",
      "GT:  What is the capital of Hawaii? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            An AI system needs to have the capability of recursive self-improvement. Once available such a capability would allow the AI system to accelerate its development at a rapid rate.\n",
      "    \n",
      "            ### Context:\n",
      "            In artificial intelligence (AI), self-improvement is a process in which an artificial agent improves its own design, often through the use of machine learning algorithms. AI agents capable of self-improvement are sometimes referred to as seed AI.\n",
      "\n",
      "Self-improvement is a central part of the AI takeoff hypothesis, a theoretical scenario in which AI progresses to superhuman intelligence. It has been speculated that if a superhuman intelligence were to be invented—either through the amplification of human intelligence or through artificial intelligence—it would vastly improve over human problem-solving and inventive skills. Such an AI is referred to as Seed A\n",
      "GT:  What is the main prerequisite for an AI to surpass human-level intelligence? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Artificial neural networks (ANNs), usually simply called neural networks (NNs) or neural nets, are computing systems inspired by the biological neural networks that constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold\n",
      "GT:  What are artificial neural networks? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            \"Colour the World\" is a song by German production group Sash! featuring Nigerian-Swedish recording artist and producer Dr. Alban. It was released in 1999 via Mighty, Club Tools, and Multiply Records as the fourth and final single from the group's second studio album, Life Goes On (1998). It was successful in a number of European countries and reached number 15 on the UK Singles Chart.\n",
      "    \n",
      "            ### Response:\n",
      "            What is the track listing of Colour the World?\n",
      "        \n",
      "GT:  What is Color of the world \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            Eggs, hard cheese, cured pork, black pepper\n",
      "        \n",
      "            ### Context:\n",
      "            Carbonara (Italian: [karboˈnaːra]) is a Roman pasta dish made with eggs, hard cheese, cured pork and black pepper. The dish took its modern form and name in the middle of the 20th century.\n",
      "\n",
      "The cheese is usually Pecorino Romano, Parmigiano-Reggiano, or a combination of the two. Spaghetti is the most common pasta, but fettuccine, rigatoni, linguine, or bucatini are also used. Normally guanciale or pancetta are used for the meat component, but lardons of smoked bacon are a common substitute outside Italy.\n",
      "    \n",
      "            ### Response:\n",
      "            What ingredients are required\n",
      "GT:  Extract the ingredients needed to make pasta carbonara. Separate them with a comma. \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            What happened in the 1984 Super Bowl?\n",
      "        \n",
      "GT:  Who was the best team in the NFL during the 1984 season? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Who was the first African American astronaut?\n",
      "        \n",
      "GT:  Who was the first African American to go to space? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            Haakon, Crown Prince of Norway (Norwegian pronunciation: [ˈhôːkʊn]; Haakon Magnus; born 20 July 1973) is the heir apparent to the Norwegian throne. He is the only son of King Harald V and Queen Sonja.\n",
      "\n",
      "Haakon represents the fourth generation of the sitting Norwegian royal family of the House of Glücksburg. He married Mette-Marit Tjessem Høiby, with whom he has two children, Princess Ingrid Alexandra and Prince Sverre Magnus.\n",
      "\n",
      "Haakon has been a member of the Young Global Leaders network, its Foundation, a Goodwill Ambassador for the United Nations, and a philanthropist. He is a trained naval officer and, as crown prince, a top military official in the Norwegian Armed Forces. He holds a BA in\n",
      "GT:  Give me a quick brief of the Prince of Norway \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            When was gradient descent discovered?\n",
      "        \n",
      "GT:  Given this text about gradient descent, what is gradient descent, and who invented it? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            What is the name of the new stadium of Tottenham Hotspur FC?\n",
      "        \n",
      "GT:  Given the reference text about Tottenham Hotspur, what was the name of the stadium that the new stadium replaced? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Why was Africa lagging behind other continents in economic growth?\n",
      "        \n",
      "GT:  What enabled Libya to obtain rapid economic growth prior to 1990? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Sex and the City is an American romantic comedy-drama television series created by Darren Star for HBO. An adaptation of Candace Bushnell's newspaper column and 1996 book anthology of the same name, the series premiered in the United States on June 6, 1998, and concluded on February 22, 2004, with 94 episodes broadcast over six seasons. Throughout its development, the series received contributions from various producers, screenwriters, and directors, principally Michael Patrick King.\n",
      "    \n",
      "            ### Response:\n",
      "            What is Sex and the City?\n",
      "        \n",
      "GT:  What is \"Sex and the City\"? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Simón José Antonio de la Santísima Trinidad Bolívar y Palacios (24 July 1783 – 17 December 1830) was a Venezuelan military and political leader who led what are currently the countries of Colombia, Venezuela, Ecuador, Peru, Panama and Bolivia to independence from the Spanish Empire. He is known colloquially as El Libertador, or the Liberator of America.\n",
      "    \n",
      "            ### Response:\n",
      "            Who is Simón José Antonio de la Santísima Trinidad Bolívar y Palacios\n",
      "        \n",
      "GT:  Given this paragraph about Simon Bolivar, tell me when and where they were born, and what they are known for \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Where is Stade Louis-II?\n",
      "        \n",
      "GT:  Where is the Stade Louis-ll located? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Ukrainian Chorus Dumka of New York was founded in 1949 with the goal \"to preserve and cultivate the rich musical heritage of Ukraine\", both for the church and for secular occasions. In the beginning, the chorus was a men's chorus of Ukrainian immigrants who met to sing music they loved. The first music director was L. Krushelnycky. The group became a mixed choir in 1959.\n",
      "\n",
      "They have performed in New York at locations including in Alice Tully Hall, Avery Fisher Hall, Brooklyn Academy of Music, Carnegie Hall, Madison Square Garden, St. Patrick's Cathedral, and Town Hall. They toured to the Kennedy Center in Washington, and in several European capitals. In 1990, the chorus toured Ukraine for the first time, singing in Kyiv, Lviv\n",
      "GT:  Using the two paragraphs below, when was the Ukrainian Chorus Dumka of NY founded, and when did it play in Ukraine for the first time? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            Please give me a bulleted list of the sub-regions of the Central Otago wine region.\n",
      "        \n",
      "GT:  What are the four major subregions in Central Otago? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Who is Lauri Johannes Silvan\n",
      "        \n",
      "GT:  Who are the people named in the passage? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            The Adventures of Tintin (French: Les Aventures de Tintin [lez‿avɑ̃tyʁ də tɛ̃tɛ̃]) is a series of 24 bande dessinée albums created by Belgian cartoonist Georges Remi, who wrote under the pen name Hergé. The series was one of the most popular European comics of the 20th century. By 2007, a century after Hergé's birth in 1907, Tintin had been published in more than 70 languages with sales of more than 200 million copies, and had been adapted for radio, television, theatre, and film.\n",
      "    \n",
      "            ### Response:\n",
      "            What is the Adventures of Tinting?\n",
      "        \n",
      "GT:  What is Adventures of Tintin? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            Sci-fi fantasy is a hybrid genre that combines tropes and elements from both science fiction and fantasy.\n",
      "        \n",
      "            ### Context:\n",
      "            Science fantasy is a hybrid genre within speculative fiction that simultaneously draws upon or combines tropes and elements from both science fiction and fantasy. In a conventional science fiction story, the world is presented as being scientifically logical; while a conventional fantasy story contains mostly supernatural and artistic elements that disregard the scientific laws of the real world. The world of science fantasy, however, is laid out to be scientifically logical and often supplied with hard science–like explanations of any supernatural elements.\n",
      "    \n",
      "            ### Response:\n",
      "            What is science fantasy?\n",
      "        \n",
      "GT:  What is Sci-fi fantasy? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            What is the Marinid Sultanate?\n",
      "        \n",
      "GT:  Who are el Marinid ? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Adams, now known as \"Bones\", is a former professional boxer who has won a world title in the Super Bantamweight weight division. He turned pro in 1990 at the age of 16.\n",
      "\n",
      "In 2000, he unanimously defeated Néstor Garza for the WBA super bantamweight title. In his first defence, he beat Andres Fernandez by 6th-round TKO. He defended the title one more time before vacating to fight Paulie Ayala. He retired in 2003 after drawing with journeyman Manuel Sepeda but returned to boxing in 2006. In 2009, he defeated veteran Alex \"Ali\" Baba in an 8th-round TKO. His last match was in 2010, a fourth-round technical knock\n",
      "GT:  Tell me about Clarence Adams \n",
      "\n",
      "===============\n",
      "GI:  2004: Twenty20 cricket\n",
      "Tendulkar was one of the first Indian players to be signed by a Twenty20 cricket franchise, the Mumbai Champs, in the Indian Premier League. He played in the 2008 season, but missed the next season due to injury. He later became a part of the Kolkata Knight Riders, and was their captain for the 2012 season. He retired from the league after the 2012 season.\n",
      "    \n",
      "            ### Context:\n",
      "            In 2002, he was signed by the Indian Premier League franchise Mumbai Champs, and he made his Twenty20 debut in 2008. He missed the 2009 season due to injury. He later played for the Kolkata Knight Riders, and was their captain for the 2012 season. He retired from\n",
      "GT:  How many runs did Sachin score in the 2011 world cup? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            What was Ski-U-Mah?\n",
      "        \n",
      "GT:  What was written in Ski-U-Mah? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Anton Pavlovich Chekhov (Russian: Антон Павлович Чехов [ɐnˈton ˈpavləvʲɪtɕ ˈtɕexəf]; 29 January 1860[note 2] – 15 July 1904[note 3]) was a Russian playwright and short-story writer who is considered to be one of the greatest writers of all time. His career as a playwright produced four classics, and his best short stories are held in high esteem by writers and critics. Along with Henrik Ibsen and August Strindberg, Chekhov is often referred to as one of the three seminal figures in the birth of early modernism in the theatre. Chekhov was a physician by profession. \"Medicine is my lawful wife\", he once said, \"and literature is my mistress\n",
      "GT:  Given this paragraph on Anton Chekhov, please tell me which playwrights he is most often grouped with? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Who is Bon Jovi?\n",
      "        \n",
      "GT:  Who is Bon Jovi \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            What does a data architect do?\n",
      "        \n",
      "GT:  summarize the responsibilities of a data architect \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            What are the risks of a Foley catheter?\n",
      "        \n",
      "GT:  Given a reference text about the adverse affects of Foley catheters, please list the main risks. \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Based on the following text, what are the main causes of climate change?\n",
      "        \n",
      "GT:  From the Passage please find out what are the cause of climate change \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            The golden age for jewish culture started somewhere around 711-718 and it was in Spain.\n",
      "        \n",
      "            ### Context:\n",
      "            A few scholars give the start of the Golden Age as 711–718, the Muslim conquest of Iberia. This is the date given by Maimonides, who is also a central figure in the period. The Muslims did not prevent Jews from following their religious beliefs, but they were not allowed to hold public office, and they were also subject to special taxation. Jews continued to practice medicine and astronomy, and the medical texts continued to be copied, translated and studied. There was also a flourishing of Jewish theological thought. The earliest works of Maimonides were composed at this time, including his Guide of the Perplexed, and his commentaries on the Mishnah and the Torah. He is generally\n",
      "GT:  When did the golden age for Jewish culture start and where was it? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Hallmarking was first observed in England in the 14th century.\n",
      "    \n",
      "            ### Response:\n",
      "            When was Hallmarking first observed in England?\n",
      "        \n",
      "GT:  From the passage provided, extract when hallmarking was first observed in England. \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Was Venetia a success for Disraeli?\n",
      "        \n",
      "GT:  Was Michael Flavin wealthy at the time he wrote Benjamin Disraeli? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            The World Bank was established in 1944. The bank is headquartered in Washington D.C., USA. Its president is David Malpass and its executive directors are David Malpass and Ceyhun Efendiar. The World Bank has 189 member countries, including the USA, Japan, China, Germany, and the UK. The bank has an annual revenue of $100 billion. The World Bank has a number of training wings, it works with the Clean Air Initiative and the UN Development Business. It works within the Open Data Initiative and hosts an Open Knowledge Repository.\n",
      "        \n",
      "            ### Context:\n",
      "            The World Bank is an international financial institution that provides loans and grants to the governments of low- and middle-income countries for the purpose of pursuing capital projects. The World Bank is the collective name for the International Bank\n",
      "GT:  From the passage note down the name of the countries which have most voting power. List the results in comma separated format. \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            The following moons were visited by Pioneer 11:\n",
      "\n",
      "Epimetheus\n",
      "Janus\n",
      "Mimas\n",
      "Titan\n",
      "    \n",
      "            ### Context:\n",
      "            Pioneer 11 was launched on April 6, 1973 and became the first spacecraft to fly past Saturn. It was launched from LC-39A at Cape Canaveral, Florida, along with its twin spacecraft Pioneer 10. It was the first spacecraft to use a radioisotope thermoelectric generator (RTG) as a power source. The RTG used the heat from the radioactive decay of plutonium to produce electricity.\n",
      "\n",
      "Pioneer 11 was the first spacecraft to pass Saturn's rings. Its flyby of Saturn and Titan on September 1, 1979, was a\n",
      "GT:  Which moons of Saturn did Pioneer 11 encounter and image? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            Yinyuan Longqi (Ingen Ryuki) the 33rd abbot of Wanfu Temple\n",
      "        \n",
      "            ### Context:\n",
      "            In 1642 he travelled to Nagasaki as a trader in Chinese medicine, and in 1644 entered Kofukuji Temple, becoming its 3rd abbot in 1645. Established by monks immigrated from China, the temple was then a base of the Ōbaku school of Zen Buddhism in Japan. In 1654 after multiple requests he succeeded in persuading Yinyuan Longqi (Ingen Ryuki), the 33rd abbot of Wanfu Temple (Mount Huangbo, Fujian) to emigrate to Japan, where he founded Ōbaku, the third and final major Japanese Zen sect.\n",
      "        \n",
      "            ### Response\n",
      "GT:  per this reference of Itsunen Shoyu, who did he convince to bring Zen Buddhism to Japan? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            The 2021 Abu Dhabi Grand Prix (officially known as the Formula 1 Etihad Airways Abu Dhabi Grand Prix 2021) was a Formula One motor race held on 12 December 2021 at the Yas Marina Circuit in Abu Dhabi, United Arab Emirates. Contested over a distance of 58 laps, the race was the twenty-second and final round of the 2021 Formula One World Championship. The race decided both the Drivers' and Constructors' championships; Max Verstappen and Lewis Hamilton both had 369.5 points coming into the race. Hamilton led most of the race and appeared on course to win what would have been a record eighth World Drivers' Championship, having taken the lead at the start of the race at turn 1 on the\n",
      "GT:  Based on the information provided, what was the controversy surrounding the 2021 Abu Dhabi Grand Prix? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Who are the influences on Jason Guettel?\n",
      "        \n",
      "GT:  List all the names of people mentioned in this passage.  Use a numbered list. \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Crashmo (Japanese: クラッシュモ, Hepburn: Kurasshu Mo), known as Fallblox in Europe and Australia and as Hiku Otsu in Japan, is a puzzle video game developed by Intelligent Systems and published by Nintendo for the Nintendo 3DS via the Nintendo eShop. The game is a sequel to Pushmo and was released in Japan on October 31, 2012, in the PAL regions on November 15, 2012, and in North America on November 22, 2012.\n",
      "    \n",
      "            ### Response:\n",
      "            Please give me some information about Crashmo\n",
      "        \n",
      "GT:  Using given text as a reference, write some details about game Crashmo \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            When he was younger, Rafa Nadal used to play with a double handed forehand. His uncle then recommended that he switch to a left-handed stroke in order to have a natural advantage on court.\n",
      "        \n",
      "GT:  Why does Nadal play tennis left-handed? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Aliasing is a phenomenon whereby a low-frequency signal gets aliased with a high-frequency signal. The aliasing effect in audio is also known as beating, which is an audible phenomenon. For example, if the sampling rate is not adequate to capture the frequency of a musical note, the note will sound like it is an octave lower.\n",
      "\n",
      "Aliasing can also occur in images. For example, if a camera takes a picture with a low resolution, the image can appear aliased, with text appearing blurry.\n",
      "    \n",
      "            ### Response:\n",
      "            What is the 'wagon wheel effect'?\n",
      "        \n",
      "GT:  Please give me an example of this phenomenon that I can understand as a layman. \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Which countries are the most successful in the Rugby World Cup?\n",
      "        \n",
      "GT:  What countries have won the rugby world cup? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Who is Carl Rimmer?\n",
      "        \n",
      "GT:  Who is Carl Rimmer \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            Given this paragraph, what are the different therapies available for autism?\n",
      "        \n",
      "GT:  What therapies are available for Autism? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Context:\n",
      "            Hair of the Dog Brewing Company is a brewery in Portland, Oregon. Several of its beers are bottle conditioned. The tasting room in southeast Portland's Buckman neighborhood closed on June 26, 2022.\n",
      "\n",
      "Alan Sprints is the owner and brewer with a few helpers, including his sister and his three sons. He graduated from Le Cordon Bleu culinary school. He was president of the home brewers club Oregon Brew Crew, prior to founding Hair of the Dog in 1993.\n",
      "\n",
      "The brewery's biggest claim to fame was its barley wine, Dave. Its Eve beer was frozen distilled three times. The brewery's Adam beer is a recreation of an Adambier, a historic brew that was originally made in Dortmund,\n",
      "GT:  Extract out some details about Hair of the Dog Brewing Company brewery from given paragraph \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            Is depression a heritable condition?\n",
      "        \n",
      "GT:  Do you think depression ran in Wittgenstein's family? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            What is the history of mountain biking?\n",
      "        \n",
      "            ### Context:\n",
      "            Mountain biking as a sport came into existence in various areas in the USA in the late 1970s and early 1980s. While many groups of riders in different US locations claim the birthright to mountain biking, it's clear that there wasn't a single person nor a single location that served as the starting point for the sport. Riders in Crested Butte, CO, Marin County, CA, Mill Valley, CA, Cupertino, CA, and several other locations contributed significantly to the birth of the sport.\n",
      "    \n",
      "            ### Response:\n",
      "            What is the history of mountain biking?\n",
      "        \n",
      "GT:  When did mountain biking start? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            Who is Robert Kiyosaki?\n",
      "        \n",
      "            ### Context:\n",
      "            Robert Toru Kiyosaki (born April 8, 1947) is an American entrepreneur, businessman and author. Kiyosaki is the founder of Rich Global LLC and the Rich Dad Company, a private financial education company that provides personal finance and business education to people through books and videos. The company's main revenues come from franchisees of the Rich Dad seminars that are conducted by independent individuals using Kiyosaki's brand name. He is also the creator of the Cashflow board and software games to educate adults and children about business and financial concepts.\n",
      "\n",
      "Kiyosaki is the author of more than 26 books, including the international self-published personal finance Rich Dad Poor Dad series of books which has been translated into\n",
      "GT:  Given this paragraph, explain who Robert Kiyosaki is? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Response:\n",
      "            What was the outcome of the 1999 Champions League final?\n",
      "        \n",
      "GT:  Tell me about the 1999 UEFA champions league final? \n",
      "\n",
      "===============\n",
      "GI:  \n",
      "            ### Input:\n",
      "            Mark Arminski is an American rock concert poster artist.  He finished his formal education studying computer generated art.\n",
      "        \n",
      "            ### Context:\n",
      "            Mark Arminski is an American rock concert poster artist born in 1950 in Detroit, Michigan. He began studying art at the Oakland Community College and pursued printmaking in stone lithography at the Kalamazoo Institute of Arts. Rounding out his formal education was his stay at the Dynamic Graphics Education Foundation in Peoria, Illinois, where he studied computer generated art.\n",
      "    \n",
      "            ### Response:\n",
      "            What kind of formal education did Mark Arminski finish?\n",
      "        \n",
      "GT:  Given this paragraph about Mark Arminski, why was he famous and what did he study last while still at school? \n",
      "\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "for index, row in df[0:50].iterrows():\n",
    "    print('GI: ', row['generated_instruction_list'])\n",
    "\n",
    "    print('GT: ',row['ground_truth_list'], '\\n')\n",
    "    print('===============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "18e69970-5f90-4a9a-926b-92ad420a8b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate==0.4.0 rouge_score==0.1.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ce941-84b9-4b29-8f27-55e71b5ef0cf",
   "metadata": {},
   "source": [
    "Compute ROUGE score for this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "161c4c35-8bc6-491d-a9af-8a7b4aab0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>0.261189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>0.148227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>0.242753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>0.244149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0     rouge1  0.261189\n",
       "1     rouge2  0.148227\n",
       "2     rougeL  0.242753\n",
       "3  rougeLsum  0.244149"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "model_results = rouge.compute(\n",
    "    predictions=generated_instruction_list,\n",
    "    references=ground_truth_list[0:len(generated_instruction_list)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('MODEL:')\n",
    "model_results_df = pd.DataFrame(model_results.items(), columns=['Metric', 'Value'])\n",
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902ce2c-3b32-43f5-b900-9260254c9d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21c03f-3b77-42c7-aa82-d3b95fc1b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1f91d-826f-4f7b-8d8f-a6e22acbe45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae1f0f-e293-4526-93da-ed10e1a7da0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f79f6-249d-4426-9fbf-4e194e404370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d9165-9add-4f45-a891-11874d773931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274672-da07-45e6-8efc-34fa1d83f6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818dfa0-18d1-4bb2-9832-5bfd6542b0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb86107-0fd9-4afe-bd97-1c2797ea871d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe148763-7e03-4483-95a8-27f3d54b9ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5606d0-7762-435e-8dca-2c0b07fbf731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
