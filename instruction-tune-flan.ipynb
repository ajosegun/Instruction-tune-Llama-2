{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053535bf-73df-4b63-9c84-356c701136ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.2\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde2a6bb-0b99-4f2c-bb62-1a2686ec8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.13.0\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 KB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.4.0\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.21.0\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 KB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes==0.40.2\n",
      "  Downloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting trl==0.4.7\n",
      "  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 KB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0) (6.0.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers==4.31.0) (2.31.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 KB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0) (23.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Using cached pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "Collecting torch>=1.13.0\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from peft==0.4.0) (5.9.5)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (3.2.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (23.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2.0.4)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting triton==2.0.0\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.4.0) (58.1.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.41.0-py3-none-any.whl (64 kB)\n",
      "Collecting lit\n",
      "  Using cached lit-16.0.6-py3-none-any.whl\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.27.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tokenizers, safetensors, pytz, mpmath, lit, cmake, bitsandbytes, xxhash, wheel, tzdata, tqdm, sympy, regex, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, networkx, multidict, fsspec, frozenlist, filelock, dill, async-timeout, yarl, pyarrow, pandas, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, multiprocess, huggingface-hub, aiosignal, transformers, nvidia-cusolver-cu11, nvidia-cudnn-cu11, aiohttp, datasets, triton, torch, accelerate, trl, peft\n",
      "Successfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.40.2 cmake-3.27.0 datasets-2.13.0 dill-0.3.6 filelock-3.12.2 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 lit-16.0.6 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.14 networkx-3.1 numpy-1.25.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-2.0.3 peft-0.4.0 pyarrow-12.0.1 pytz-2023.3 regex-2023.6.3 safetensors-0.3.1 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.31.0 triton-2.0.0 trl-0.4.7 tzdata-2023.3 wheel-0.41.0 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/environment/ai-project/venv/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgradev -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d5c91-e135-4452-960e-8e37aa4fa54d",
   "metadata": {},
   "source": [
    "#### fix for could not import -lmza\n",
    "https://github.com/lucidrains/imagen-pytorch/issues/92 \n",
    "\n",
    "#### Fix for Can't import datasets AttributeError: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)\n",
    "\n",
    "Restart juypyter and kernel\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d9cd78-87e0-4456-81f7-e94932af0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/environment/ai-project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset json (/home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb913e1-0e58-4a35-8630-8e87fd15cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> instruction\n",
      "====================\n",
      "Is plastic bad for us?\n",
      "====================\n",
      "--> context\n",
      "====================\n",
      "\n",
      "====================\n",
      "--> response\n",
      "====================\n",
      "Have you ever thought about where your plastic water bottles go? It probably ends up in a landfill, where it will decompose for over 1000 years as it deposits toxic chemicals. Plastic is an invention that has done more harm than good. There are about 363,762,732,605 pounds of plastic covering the earth. Most of this plastic can’t be recycled. Although people think it is too expensive or inconvenient to get rid of plastics, we should reduce the amount of plastic used or increase the price of plastics. One reason is that when plastic invades a balanced ecosystem, it corrupts it completely. The marine biome is one that is affected by this product. The animals in the ocean die of suffocation, ingestion of plastics, and entanglement. The animals affected are seagulls, whales, seals, fish, and turtles, and that's just a few. The animals eat so much plastic that they die of constipation from it. Another reason is the presence of microplastics in food and people. About 99% of fish have microplastics in their stomachs, livers, and filets (the meat of fish). Approximately 80% of cows have microplastics in their meat and milk. A good point is that mealworms and waxworms are studied to digest plastics, and they should be added to landfills to break down plastics. There are about 50,000 particles of plastic in an adult person, and this number is rising fast. Third, most plastic is littered. There are about 10 million tons of plastic litter every year. Every week, you ingest about a credit card's worth of plastic. There are about 44 million pounds of plastic that are produced that is very deadly to the environment.\"The plastic in the environment is in your water, food, and body. These plastics should not be there; they can lead to cancer, according to a recent study. We need to stop using plastic in our lives. Some ways we can reduce the amount of plastic on earth are by using sustainable materials such as wood, plant fibers, and mushroom styrofoam. We can also try to buy recyclable plastics that have a triangle formed by three cycling arrows. Use a tote bag instead of a plastic one, or a metal and glass straw instead of a plastic one. You can also use a metal cutlery kit instead of plastic. These are all ways you can make a difference in your everyday life.\n",
      "====================\n",
      "--> category\n",
      "====================\n",
      "general_qa\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "sample_ds = dataset[randrange(len(dataset))]\n",
    "\n",
    "for key, value in sample_ds.items():\n",
    "    print(f\"--> {key}\")\n",
    "    print(\"====================\")\n",
    "    print(value)\n",
    "    print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fe73ef-bd68-429a-8adb-080314586b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brainstorming',\n",
       " 'classification',\n",
       " 'closed_qa',\n",
       " 'creative_writing',\n",
       " 'general_qa',\n",
       " 'information_extraction',\n",
       " 'open_qa',\n",
       " 'summarization'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3529697a-f1d5-410e-a813-23fee88ba9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset.map( ['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48905876-1a02-40cb-b4c6-1e8f09b8130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(filter(lambda x :x != '', list(map(lambda x:((x['category'], len(x['context'])) if len(x['context']) > 5000 else ''), dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fbfd21-dd46-4044-95d1-f91d4f3374cb",
   "metadata": {},
   "source": [
    "To instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a formatting_function that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b8ff2b8-d4e0-40af-ad22-06442d57a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "            \n",
    "            ### Input:\n",
    "            {sample['response']}\n",
    "            \n",
    "            ### Response:\n",
    "            {sample['instruction']}\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25d8e6-e589-48a3-9aee-6b03d66389c4",
   "metadata": {},
   "source": [
    "Let's test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e89814-1a71-474c-babc-9c398972125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
      "            \n",
      "            ### Input:\n",
      "            The most important concept around climate change is that \"weather\", such as snowstorms and blizzards, is different then \"climate\". With climate change, the extremes are what fluctuate most: the colds are colder, and the hots are hotter. This sounds like your experience with blizzards and snowstorms. You may actually be experiencing more extreme cold weather as the planet warms due to greenhouse gases because of how the jet-stream is changing and warmer ocean temperatures put more water vapor into the atmosphere. Although it may seem that the weather is still cold at a local level, nearly every scientist agrees that climate change is causing more dramatic and extreme weather all around the world.\n",
      "            \n",
      "            ### Response:\n",
      "            How can the climate be warming if I keep getting blizzards in my town?\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_instruction(dataset[randrange(len(dataset))]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9519a9-759d-4a77-92f7-c7f4b8534bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "            Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
      "            \n",
      "            ### Input:\n",
      "            Have you ever thought about where your plastic water bottles go? It probably ends up in a landfill, where it will decompose for over 1000 years as it deposits toxic chemicals. Plastic is an invention that has done more harm than good. There are about 363,762,732,605 pounds of plastic covering the earth. Most of this plastic can’t be recycled. Although people think it is too expensive or inconvenient to get rid of plastics, we should reduce the amount of plastic used or increase the price of plastics. One reason is that when plastic invades a balanced ecosystem, it corrupts it completely. The marine biome is one that is affected by this product. The animals in the ocean die of suffocation, ingestion of plastics, and entanglement. The animals affected are seagulls, whales, seals, fish, and turtles, and that's just a few. The animals eat so much plastic that they die of constipation from it. Another reason is the presence of microplastics in food and people. About 99% of fish have microplastics in their stomachs, livers, and filets (the meat of fish). Approximately 80% of cows have microplastics in their meat and milk. A good point is that mealworms and waxworms are studied to digest plastics, and they should be added to landfills to break down plastics. There are about 50,000 particles of plastic in an adult person, and this number is rising fast. Third, most plastic is littered. There are about 10 million tons of plastic litter every year. Every week, you ingest about a credit card's worth of plastic. There are about 44 million pounds of plastic that are produced that is very deadly to the environment.\"The plastic in the environment is in your water, food, and body. These plastics should not be there; they can lead to cancer, according to a recent study. We need to stop using plastic in our lives. Some ways we can reduce the amount of plastic on earth are by using sustainable materials such as wood, plant fibers, and mushroom styrofoam. We can also try to buy recyclable plastics that have a triangle formed by three cycling arrows. Use a tote bag instead of a plastic one, or a metal and glass straw instead of a plastic one. You can also use a metal cutlery kit instead of plastic. These are all ways you can make a difference in your everyday life.\n",
      "            \n",
      "            ### Response:\n",
      "            Is plastic bad for us?\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(format_instruction(sample_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0156c-85de-4e8a-aeb0-8f7f8d785212",
   "metadata": {},
   "source": [
    "### Instruction-tune Llama 2 using trl and the SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec40fdb-1621-42a2-901c-71b8a287d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "# COMMENT IN TO USE FLASH ATTENTION\n",
    "# replace attention with flash attention\n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
    "#     print(\"Using flash attention\")\n",
    "#     replace_attn_with_flash_attn()\n",
    "#     use_flash_attention = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d29288-3392-4e22-952b-d39d271af9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Downloading model.safetensors:   0%|                                                                                 | 0.00/3.13G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   1%|▋                                                                        | 31.5M/3.13G [00:00<00:14, 221MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   2%|█▍                                                                       | 62.9M/3.13G [00:00<00:13, 233MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   3%|██▏                                                                      | 94.4M/3.13G [00:00<00:17, 173MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   4%|██▋                                                                       | 115M/3.13G [00:00<00:16, 179MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   5%|███▋                                                                      | 157M/3.13G [00:00<00:12, 238MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   6%|████▋                                                                     | 199M/3.13G [00:00<00:10, 275MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   7%|█████▍                                                                    | 231M/3.13G [00:01<00:12, 228MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   8%|██████▏                                                                   | 262M/3.13G [00:01<00:17, 165MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   9%|██████▋                                                                   | 283M/3.13G [00:01<00:16, 168MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  10%|███████▏                                                                  | 304M/3.13G [00:01<00:16, 171MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  11%|███████▉                                                                  | 336M/3.13G [00:01<00:14, 194MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  12%|████████▋                                                                 | 367M/3.13G [00:01<00:12, 218MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  13%|█████████▍                                                                | 398M/3.13G [00:01<00:12, 217MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  14%|██████████▏                                                               | 430M/3.13G [00:02<00:12, 218MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  15%|██████████▉                                                               | 461M/3.13G [00:02<00:11, 232MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  16%|███████████▋                                                              | 493M/3.13G [00:02<00:10, 247MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  17%|████████████▍                                                             | 524M/3.13G [00:02<00:12, 204MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  18%|█████████████▏                                                            | 556M/3.13G [00:02<00:18, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  18%|█████████████▌                                                            | 577M/3.13G [00:03<00:17, 143MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  19%|██████████████▎                                                           | 608M/3.13G [00:03<00:16, 156MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  20%|███████████████                                                           | 640M/3.13G [00:03<00:13, 181MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  21%|███████████████▊                                                          | 671M/3.13G [00:03<00:13, 187MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  22%|████████████████▌                                                         | 703M/3.13G [00:03<00:11, 205MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  23%|█████████████████▎                                                        | 734M/3.13G [00:04<00:17, 139MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  24%|██████████████████                                                        | 765M/3.13G [00:04<00:14, 163MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  25%|██████████████████▌                                                       | 786M/3.13G [00:04<00:14, 162MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  26%|███████████████████▎                                                      | 818M/3.13G [00:04<00:12, 185MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  27%|████████████████████                                                      | 849M/3.13G [00:04<00:11, 201MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  28%|████████████████████▊                                                     | 881M/3.13G [00:04<00:11, 202MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  29%|█████████████████████▌                                                    | 912M/3.13G [00:04<00:12, 184MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  30%|██████████████████████▎                                                   | 944M/3.13G [00:05<00:12, 171MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  31%|██████████████████████▊                                                   | 965M/3.13G [00:05<00:12, 177MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  31%|███████████████████████▎                                                  | 986M/3.13G [00:05<00:11, 182MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  32%|███████████████████████▋                                                 | 1.02G/3.13G [00:05<00:10, 209MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  33%|████████████████████████▍                                                | 1.05G/3.13G [00:05<00:09, 211MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  34%|█████████████████████████▏                                               | 1.08G/3.13G [00:05<00:10, 189MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  35%|█████████████████████████▉                                               | 1.11G/3.13G [00:05<00:09, 212MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  36%|██████████████████████████▋                                              | 1.14G/3.13G [00:06<00:10, 196MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  37%|███████████████████████████▎                                             | 1.17G/3.13G [00:06<00:09, 211MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  38%|████████████████████████████                                             | 1.21G/3.13G [00:06<00:08, 219MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  39%|████████████████████████████▊                                            | 1.24G/3.13G [00:06<00:14, 133MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  40%|████████████████████████████▉                                           | 1.26G/3.13G [00:07<00:19, 94.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  41%|█████████████████████████████▍                                          | 1.28G/3.13G [00:07<00:19, 93.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  42%|██████████████████████████████▌                                          | 1.31G/3.13G [00:07<00:16, 112MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  43%|███████████████████████████████                                          | 1.33G/3.13G [00:07<00:15, 113MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  43%|███████████████████████████████▌                                         | 1.35G/3.13G [00:07<00:13, 128MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  44%|████████████████████████████████▎                                        | 1.38G/3.13G [00:08<00:11, 153MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  45%|████████████████████████████████▉                                        | 1.42G/3.13G [00:08<00:10, 162MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  46%|█████████████████████████████████▍                                       | 1.44G/3.13G [00:08<00:10, 166MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  47%|██████████████████████████████████▏                                      | 1.47G/3.13G [00:08<00:08, 188MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  48%|██████████████████████████████████▉                                      | 1.50G/3.13G [00:08<00:07, 213MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  49%|███████████████████████████████████▋                                     | 1.53G/3.13G [00:08<00:07, 220MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  50%|████████████████████████████████████▍                                    | 1.56G/3.13G [00:08<00:07, 209MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  51%|█████████████████████████████████████▏                                   | 1.59G/3.13G [00:09<00:08, 183MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  52%|█████████████████████████████████████▋                                   | 1.61G/3.13G [00:09<00:08, 183MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  52%|██████████████████████████████████████                                   | 1.64G/3.13G [00:09<00:08, 175MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  53%|██████████████████████████████████████▊                                  | 1.67G/3.13G [00:09<00:07, 193MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  54%|███████████████████████████████████████▌                                 | 1.70G/3.13G [00:09<00:06, 209MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  55%|████████████████████████████████████████▎                                | 1.73G/3.13G [00:09<00:06, 217MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  56%|█████████████████████████████████████████                                | 1.76G/3.13G [00:09<00:06, 221MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  57%|█████████████████████████████████████████▊                               | 1.79G/3.13G [00:09<00:06, 215MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  58%|██████████████████████████████████████████▌                              | 1.82G/3.13G [00:10<00:06, 197MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  59%|███████████████████████████████████████████                              | 1.85G/3.13G [00:10<00:08, 159MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  60%|███████████████████████████████████████████▍                             | 1.87G/3.13G [00:10<00:08, 145MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  61%|████████████████████████████████████████████▏                            | 1.90G/3.13G [00:10<00:07, 163MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  62%|████████████████████████████████████████████▉                            | 1.93G/3.13G [00:10<00:06, 175MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  63%|█████████████████████████████████████████████▋                           | 1.96G/3.13G [00:11<00:06, 185MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  64%|██████████████████████████████████████████████▍                          | 1.99G/3.13G [00:11<00:05, 206MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  65%|███████████████████████████████████████████████▏                         | 2.02G/3.13G [00:11<00:05, 200MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  65%|███████████████████████████████████████████████▋                         | 2.04G/3.13G [00:11<00:05, 196MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  66%|████████████████████████████████████████████████▏                        | 2.07G/3.13G [00:11<00:07, 133MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  67%|████████████████████████████████████████████████▊                        | 2.10G/3.13G [00:11<00:06, 158MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  68%|█████████████████████████████████████████████████▎                       | 2.12G/3.13G [00:11<00:06, 157MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  68%|█████████████████████████████████████████████████▊                       | 2.14G/3.13G [00:12<00:07, 136MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  69%|██████████████████████████████████████████████████▎                      | 2.16G/3.13G [00:12<00:06, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  70%|███████████████████████████████████████████████████                      | 2.19G/3.13G [00:12<00:05, 160MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  71%|███████████████████████████████████████████████████▌                     | 2.21G/3.13G [00:12<00:05, 165MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  72%|████████████████████████████████████████████████████▎                    | 2.24G/3.13G [00:12<00:04, 191MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  72%|████████████████████████████████████████████████████▊                    | 2.26G/3.13G [00:12<00:04, 195MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  73%|█████████████████████████████████████████████████████▌                   | 2.30G/3.13G [00:12<00:03, 214MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  74%|██████████████████████████████████████████████████████▏                  | 2.33G/3.13G [00:13<00:03, 237MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  75%|██████████████████████████████████████████████████████▉                  | 2.36G/3.13G [00:13<00:03, 195MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  76%|███████████████████████████████████████████████████████▋                 | 2.39G/3.13G [00:13<00:04, 181MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  77%|████████████████████████████████████████████████████████▏                | 2.41G/3.13G [00:13<00:04, 174MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  78%|████████████████████████████████████████████████████████▉                | 2.44G/3.13G [00:13<00:03, 191MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  79%|█████████████████████████████████████████████████████████▍               | 2.46G/3.13G [00:13<00:03, 179MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  80%|██████████████████████████████████████████████████████████▏              | 2.50G/3.13G [00:14<00:03, 163MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  80%|██████████████████████████████████████████████████████████▋              | 2.52G/3.13G [00:14<00:04, 154MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  81%|███████████████████████████████████████████████████████████▍             | 2.55G/3.13G [00:14<00:03, 175MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  82%|███████████████████████████████████████████████████████████▊             | 2.57G/3.13G [00:14<00:04, 124MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  83%|████████████████████████████████████████████████████████████▎            | 2.59G/3.13G [00:14<00:04, 120MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  83%|████████████████████████████████████████████████████████████▊            | 2.61G/3.13G [00:15<00:03, 134MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  84%|█████████████████████████████████████████████████████████████▌           | 2.64G/3.13G [00:15<00:03, 153MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  85%|██████████████████████████████████████████████████████████████           | 2.66G/3.13G [00:15<00:03, 151MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  86%|██████████████████████████████████████████████████████████████▌          | 2.68G/3.13G [00:15<00:02, 163MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  86%|███████████████████████████████████████████████████████████████          | 2.71G/3.13G [00:15<00:02, 167MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  87%|███████████████████████████████████████████████████████████████▊         | 2.74G/3.13G [00:15<00:02, 189MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  88%|████████████████████████████████████████████████████████████████▎        | 2.76G/3.13G [00:15<00:01, 190MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  89%|████████████████████████████████████████████████████████████████▉        | 2.79G/3.13G [00:15<00:01, 205MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  90%|█████████████████████████████████████████████████████████████████▍       | 2.81G/3.13G [00:16<00:01, 178MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  90%|█████████████████████████████████████████████████████████████████▉       | 2.83G/3.13G [00:16<00:02, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  91%|██████████████████████████████████████████████████████████████████▋      | 2.86G/3.13G [00:16<00:01, 166MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  92%|███████████████████████████████████████████████████████████████████▍     | 2.89G/3.13G [00:16<00:01, 183MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  93%|███████████████████████████████████████████████████████████████████▉     | 2.92G/3.13G [00:16<00:01, 162MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  94%|████████████████████████████████████████████████████████████████████▍    | 2.94G/3.13G [00:16<00:01, 148MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  94%|████████████████████████████████████████████████████████████████████▉    | 2.96G/3.13G [00:17<00:01, 160MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  95%|█████████████████████████████████████████████████████████████████████▍   | 2.98G/3.13G [00:17<00:00, 170MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  96%|█████████████████████████████████████████████████████████████████████▉   | 3.00G/3.13G [00:17<00:00, 153MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  97%|██████████████████████████████████████████████████████████████████████▌  | 3.03G/3.13G [00:17<00:00, 186MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  97%|███████████████████████████████████████████████████████████████████████  | 3.05G/3.13G [00:17<00:00, 166MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  98%|███████████████████████████████████████████████████████████████████████▌ | 3.07G/3.13G [00:17<00:00, 112MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors:  99%|████████████████████████████████████████████████████████████████████████ | 3.09G/3.13G [00:18<00:00, 129MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████| 3.13G/3.13G [00:18<00:00, 172MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)of-00005.safetensors:  50%|█████████████████████████████████                                 | 4.74G/9.45G [04:14<04:13, 18.6MB/s]\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Downloading (…)neration_config.json: 100%|███████████████████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 920kB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face model id\n",
    "# model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "model_id = \"google/flan-t5-large\"\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cad2c374-8b4e-4c9d-85ef-5aa324f2ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████████████████████████████| 2.54k/2.54k [00:00<00:00, 15.8MB/s]\u001b[A\n",
      "\n",
      "Downloading spiece.model: 100%|████████████████████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 736MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)/main/tokenizer.json:   0%|                                                                           | 0.00/2.42M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████████████████████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 8.86MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████| 2.20k/2.20k [00:00<00:00, 13.3MB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Validate that the model is using flash attention, by comparing doc strings\n",
    "if use_flash_attention:\n",
    "    from utils.llama_patch import forward\n",
    "    assert model.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d93ab-804b-44fb-aae7-688ffb2fdd29",
   "metadata": {},
   "source": [
    "The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921a7911-62e9-4c90-a053-96f786ec5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697f936-cff2-4bc0-9b44-0011f0b97142",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb305bf0-0f0d-4fff-9f40-9933033a35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"flan-7-int4-dolly\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b930587-788c-4ee4-b86c-97dc5a9595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e1c7683-3fde-4d93-b3cf-5b1b474120ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 21.99 GiB total capacity; 10.84 GiB already allocated; 659.75 MiB free; 11.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# there will not be a progress bar since tqdm is disabled\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:141\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m     detached_inputs \u001b[38;5;241m=\u001b[39m detach_variable(\u001b[38;5;28mtuple\u001b[39m(inputs))\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad(), \\\n\u001b[1;32m    139\u001b[0m          torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mgpu_autocast_kwargs), \\\n\u001b[1;32m    140\u001b[0m          torch\u001b[38;5;241m.\u001b[39mcpu\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mcpu_autocast_kwargs):\n\u001b[0;32m--> 141\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdetached_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    144\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (outputs,)\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1077\u001b[0m, in \u001b[0;36mT5Stack.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[0;32m-> 1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:724\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:635\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    633\u001b[0m ):\n\u001b[1;32m    634\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 635\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    647\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/environment/ai-project/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:561\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    558\u001b[0m     position_bias_masked \u001b[38;5;241m=\u001b[39m position_bias\n\u001b[1;32m    560\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[0;32m--> 561\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[1;32m    562\u001b[0m     scores\n\u001b[1;32m    563\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    564\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    565\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    566\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 21.99 GiB total capacity; 10.84 GiB already allocated; 659.75 MiB free; 11.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b47bd-0af6-46eb-9843-18fcc8886da9",
   "metadata": {},
   "source": [
    "## Test Model and run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8839a-2449-4b22-8099-2ce24be47fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flash_attention:\n",
    "    # unpatch flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "args.output_dir = \"llama-7-int4-dolly\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dd234-e0ec-4acb-88d5-532176e2b3ae",
   "metadata": {},
   "source": [
    "Let’s load the dataset again with a random sample to try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae66a3a-e612-4299-a043-056193422b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd5fe5-8b61-4fb8-baaf-aa8d1a85db0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b7474-b307-416f-a45c-7d1cdb14e299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae1f0f-e293-4526-93da-ed10e1a7da0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f79f6-249d-4426-9fbf-4e194e404370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d9165-9add-4f45-a891-11874d773931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274672-da07-45e6-8efc-34fa1d83f6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818dfa0-18d1-4bb2-9832-5bfd6542b0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb86107-0fd9-4afe-bd97-1c2797ea871d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe148763-7e03-4483-95a8-27f3d54b9ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5606d0-7762-435e-8dca-2c0b07fbf731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
